{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_Exercises.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu5F/boAAsckP9fvZbPZOp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeh4amvQsD50"
      },
      "source": [
        "#Setting Up Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHr-lEW2MGLu"
      },
      "source": [
        "#Books:\n",
        "\n",
        "Learn PySpark: Build Python-based Machine Learning and Deep Learning Models by Promod Singh/ Apress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXxsYDZPMGPF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXCJfBohwiKw",
        "outputId": "aa2977b1-8913-4da1-9f81-a57cb36b8b4f"
      },
      "source": [
        "!git clone https://github.com/Apress/learn-pyspark.git\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://mirrors.estointernet.in/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf '/content/spark-3.1.2-bin-hadoop3.2.tgz'\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'learn-pyspark'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 44 (delta 11), reused 44 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mugElNNJK9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b625507-686b-40af-ac77-d73f3e6a39ae"
      },
      "source": [
        "#Spark env seyup\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "print('spark location -->', findspark.find())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark location --> /content/spark-3.1.2-bin-hadoop3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHGUcd-mJCG6"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('atom').getOrCreate()\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5xfeKfJ_AX"
      },
      "source": [
        "#Creating Dataframe\n",
        "schema = StructType().add(\"user_id\",\"string\").add(\"country\",\"string\").add(\"browser\",\"string\").add(\"os\",\"string\").add(\"age\",\"integer\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAzVmFKSKEQL"
      },
      "source": [
        "df = spark.createDataFrame([('A201','India','Chrome','WIN',33),('A202','UK','Safari','MAC',27),\n",
        "                            ('A203','US','Firefox','UBUNTU',2)],schema=schema)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPSMflVkKMsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac03a271-2f54-459f-e242-71dd6ceb7994"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- browser: string (nullable = true)\n",
            " |-- os: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBz2O3uYKOni",
        "outputId": "e95e03ee-fa9f-4c32-ae81-fb15a91312ef"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+-------+------+---+\n",
            "|user_id|country|browser|    os|age|\n",
            "+-------+-------+-------+------+---+\n",
            "|   A201|  India| Chrome|   WIN| 33|\n",
            "|   A202|     UK| Safari|   MAC| 27|\n",
            "|   A203|     US|Firefox|UBUNTU|  2|\n",
            "+-------+-------+-------+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_7mE3HSKQoF"
      },
      "source": [
        "df_na = spark.createDataFrame([(\"A203\",None,\"Chrome\",\"WIN\",33),(\"A201\",'China',None,\"MacOS\",35),(\"A205\",'UK',\"Mozilla\",\"Linux\",25)],\n",
        "                              schema=schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrKImMq-KS6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5348e6-7239-4672-82d3-19cd5ca03b87"
      },
      "source": [
        "df_na.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+-------+-----+---+\n",
            "|user_id|country|browser|   os|age|\n",
            "+-------+-------+-------+-----+---+\n",
            "|   A203|   null| Chrome|  WIN| 33|\n",
            "|   A201|  China|   null|MacOS| 35|\n",
            "|   A205|     UK|Mozilla|Linux| 25|\n",
            "+-------+-------+-------+-----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykr_RJd9vcrs",
        "outputId": "8375b780-fd3e-45e9-9d73-5e9d38981476"
      },
      "source": [
        "df_na.fillna('0').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+-------+-----+---+\n",
            "|user_id|country|browser|   os|age|\n",
            "+-------+-------+-------+-----+---+\n",
            "|   A203|      0| Chrome|  WIN| 33|\n",
            "|   A201|  China|      0|MacOS| 35|\n",
            "|   A205|     UK|Mozilla|Linux| 25|\n",
            "+-------+-------+-------+-----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwqw7JezvjVt",
        "outputId": "a61585fd-5ef5-46ea-b2a5-8021d23e086d"
      },
      "source": [
        "df_na.fillna({'country':'USA',\"browser\":\"Safari\"}).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+-------+-----+---+\n",
            "|user_id|country|browser|   os|age|\n",
            "+-------+-------+-------+-----+---+\n",
            "|   A203|    USA| Chrome|  WIN| 33|\n",
            "|   A201|  China| Safari|MacOS| 35|\n",
            "|   A205|     UK|Mozilla|Linux| 25|\n",
            "+-------+-------+-------+-----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA2xC9gavxvL",
        "outputId": "41d6b707-9fcb-4827-a693-4033f518d77d"
      },
      "source": [
        "df_na.na.drop().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+-------+-----+---+\n",
            "|user_id|country|browser|   os|age|\n",
            "+-------+-------+-------+-----+---+\n",
            "|   A205|     UK|Mozilla|Linux| 25|\n",
            "+-------+-------+-------+-----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q6jQtDzwC6l"
      },
      "source": [
        "df = spark.read.csv(r'/content/learn-pyspark/chap_2/customer_data.csv',header=True,inferSchema=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V24bqju8w6Gq",
        "outputId": "c33623c8-297a-4138-8d53-72c81a17be54"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmbLc3OWw7xx",
        "outputId": "07a84516-40f1-4f1b-9b5b-d6695788db8f"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0|\n",
            "|Modern, complete ...|               1|                 3|40-50 years|      Average Family|     19504|    0|\n",
            "|  Large family farms|               1|                 4|30-40 years|             Farmers|     34943|    0|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzz3-ltJw9wx",
        "outputId": "dbaa3533-87cd-442f-e223-f0ee3ee4e87d"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Customer_subtype: string (nullable = true)\n",
            " |-- Number_of_houses: integer (nullable = true)\n",
            " |-- Avg_size_household: integer (nullable = true)\n",
            " |-- Avg_age: string (nullable = true)\n",
            " |-- Customer_main_type: string (nullable = true)\n",
            " |-- Avg_Salary: integer (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbSQxBQpxDV-",
        "outputId": "34f17be7-a9dd-482f-8339-c87bb643676f"
      },
      "source": [
        "df.summary().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+------------------+------------------+-----------+--------------------+-----------------+------------------+\n",
            "|summary|    Customer_subtype|  Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|       Avg_Salary|             label|\n",
            "+-------+--------------------+------------------+------------------+-----------+--------------------+-----------------+------------------+\n",
            "|  count|                2000|              2000|              2000|       2000|                2000|             2000|              2000|\n",
            "|   mean|                null|            1.1075|            2.6895|       null|                null|     1616908.0835|            0.0605|\n",
            "| stddev|                null|0.3873225521186316|0.7914562220841646|       null|                null|6822647.757312146|0.2384705099001677|\n",
            "|    min|Affluent senior a...|                 1|                 1|20-30 years|      Average Family|             1361|                 0|\n",
            "|    25%|                null|                 1|                 2|       null|                null|            20315|                 0|\n",
            "|    50%|                null|                 1|                 3|       null|                null|            31421|                 0|\n",
            "|    75%|                null|                 1|                 3|       null|                null|            42949|                 0|\n",
            "|    max| Young, low educated|                10|                 5|70-80 years|Successful hedonists|         48919896|                 1|\n",
            "+-------+--------------------+------------------+------------------+-----------+--------------------+-----------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhFKNBiXxdAe"
      },
      "source": [
        "#Subset of a Dataframe\n",
        "    Select\n",
        "\n",
        "    Filter\n",
        "\n",
        "    Where"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhq_-d3yxKHe",
        "outputId": "ca0c1751-0ff6-4807-cc20-e5965b84a9ee"
      },
      "source": [
        "df.select(['Customer_subtype','Avg_Salary']).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|    Customer_subtype|Avg_Salary|\n",
            "+--------------------+----------+\n",
            "|Lower class large...|     44905|\n",
            "|Mixed small town ...|     37575|\n",
            "|Mixed small town ...|     27915|\n",
            "+--------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYNhCPQyxyez",
        "outputId": "88b5eb29-17f5-4d36-e6a6-ae70990907fc"
      },
      "source": [
        "df.filter(df['Avg_Salary']>100000).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORFPPh-fyBrl",
        "outputId": "6e4b1b3d-330f-4cbc-b9ce-1ac8efcb8961"
      },
      "source": [
        "df.filter(df['Avg_Salary']>100000).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "| High status seniors|               1|                 3|40-50 years|Successful hedonists|   4670288|    0|\n",
            "|Affluent young fa...|               1|                 3|30-40 years|      Average Family|    762769|    1|\n",
            "| High status seniors|               1|                 3|50-60 years|Successful hedonists|   9561873|    0|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNkMqZmQyFvU",
        "outputId": "a708b089-ea5f-44d7-f857-11b90620fcdc"
      },
      "source": [
        "df.where((df['Avg_Salary'] > 100000) & (df['Number_of_houses']>2)).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|Affluent senior a...|               3|                 2|50-60 years|Successful hedonists|    596723|    0|\n",
            "|Affluent senior a...|               3|                 2|50-60 years|Successful hedonists|    944444|    0|\n",
            "|Affluent senior a...|               3|                 2|50-60 years|Successful hedonists|    788477|    0|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23fkUL8zyhCw"
      },
      "source": [
        "#Aggregations\n",
        "Any kind of aggregation can be broken simply into three stages, in the following order:\n",
        "\n",
        "    Split\n",
        "\n",
        "    Apply\n",
        "      mean,maxmmin,sum\n",
        "\n",
        "    Combine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fKBJa3Ryank",
        "outputId": "4883ee5b-eb0a-49c0-bba1-875cbdc7ebbc"
      },
      "source": [
        "for col in df.columns:\n",
        "  if col != 'Avg_Salary':\n",
        "    print(f\"Aggregation for {col}\")\n",
        "    df.groupBy(col).count().orderBy('count',ascending=False).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aggregation for Customer_subtype\n",
            "+------------------------------------------+-----+\n",
            "|Customer_subtype                          |count|\n",
            "+------------------------------------------+-----+\n",
            "|Lower class large families                |288  |\n",
            "|Traditional families                      |129  |\n",
            "|Middle class families                     |122  |\n",
            "|Large religious families                  |107  |\n",
            "|Modern, complete families                 |93   |\n",
            "|Couples with teens 'Married with children'|83   |\n",
            "|Young and rising                          |78   |\n",
            "|High status seniors                       |76   |\n",
            "|Low income catholics                      |72   |\n",
            "|Mixed seniors                             |71   |\n",
            "|Village families                          |68   |\n",
            "|Mixed rurals                              |67   |\n",
            "|Stable family                             |62   |\n",
            "|Young all american family                 |62   |\n",
            "|Young, low educated                       |56   |\n",
            "|Large family, employed child              |56   |\n",
            "|Family starters                           |55   |\n",
            "|High Income, expensive child              |52   |\n",
            "|Mixed small town dwellers                 |47   |\n",
            "|Religious elderly singles                 |47   |\n",
            "+------------------------------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "Aggregation for Number_of_houses\n",
            "+----------------+-----+\n",
            "|Number_of_houses|count|\n",
            "+----------------+-----+\n",
            "|1               |1808 |\n",
            "|2               |178  |\n",
            "|3               |12   |\n",
            "|5               |1    |\n",
            "|10              |1    |\n",
            "+----------------+-----+\n",
            "\n",
            "Aggregation for Avg_size_household\n",
            "+------------------+-----+\n",
            "|Avg_size_household|count|\n",
            "+------------------+-----+\n",
            "|3                 |900  |\n",
            "|2                 |730  |\n",
            "|4                 |255  |\n",
            "|1                 |94   |\n",
            "|5                 |21   |\n",
            "+------------------+-----+\n",
            "\n",
            "Aggregation for Avg_age\n",
            "+-----------+-----+\n",
            "|Avg_age    |count|\n",
            "+-----------+-----+\n",
            "|40-50 years|1028 |\n",
            "|30-40 years|496  |\n",
            "|50-60 years|373  |\n",
            "|60-70 years|64   |\n",
            "|20-30 years|31   |\n",
            "|70-80 years|8    |\n",
            "+-----------+-----+\n",
            "\n",
            "Aggregation for Customer_main_type\n",
            "+---------------------+-----+\n",
            "|Customer_main_type   |count|\n",
            "+---------------------+-----+\n",
            "|Family with grown ups|542  |\n",
            "|Average Family       |308  |\n",
            "|Conservative families|236  |\n",
            "|Retired and Religious|202  |\n",
            "|Successful hedonists |194  |\n",
            "|Living well          |178  |\n",
            "|Driven Growers       |172  |\n",
            "|Farmers              |93   |\n",
            "|Cruising Seniors     |60   |\n",
            "|Career Loners        |15   |\n",
            "+---------------------+-----+\n",
            "\n",
            "Aggregation for label\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|0    |1879 |\n",
            "|1    |121  |\n",
            "+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo2WWIYLy9QL",
        "outputId": "208ddba0-6610-477b-a649-ed015b27c048"
      },
      "source": [
        "df.groupBy('Customer_main_type').agg(F.mean('Avg_Salary')).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------------------+\n",
            "|  Customer_main_type|   avg(Avg_Salary)|\n",
            "+--------------------+------------------+\n",
            "|             Farmers|30209.333333333332|\n",
            "|       Career Loners|           32272.6|\n",
            "|Retired and Relig...| 27338.80693069307|\n",
            "+--------------------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsO68xXLz8Ag",
        "outputId": "087f19a3-2a37-442e-93fa-83c3309cfc5f"
      },
      "source": [
        "df.sort(\"Avg_Salary\",ascending=False).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "| High status seniors|               1|                 2|60-70 years|Successful hedonists|  48919896|    0|\n",
            "|High Income, expe...|               1|                 2|50-60 years|Successful hedonists|  48177970|    0|\n",
            "|High Income, expe...|               1|                 2|50-60 years|Successful hedonists|  48069548|    1|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfMsdxvI0RAU",
        "outputId": "2d5df06f-956f-47d7-a0c7-ec7325ec2101"
      },
      "source": [
        "df.groupBy('Customer_subtype').agg(F.mean('Avg_Salary').alias('mean_salary')).orderBy('mean_salary',ascending=False).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|    Customer_subtype|         mean_salary|\n",
            "+--------------------+--------------------+\n",
            "| High status seniors| 2.507677857894737E7|\n",
            "|High Income, expe...|2.3839817807692308E7|\n",
            "|Affluent young fa...|   662068.7777777778|\n",
            "+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HM8Gj6R1BAd"
      },
      "source": [
        "Collect\n",
        "      \n",
        "    Collect List\n",
        "\n",
        "    Collect Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5tGq4if0xIq",
        "outputId": "cd5e3666-617c-4e7a-9d52-ccc816f5079b"
      },
      "source": [
        "df.groupby('Customer_subtype').agg(F.collect_set(\"Number_of_houses\")).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----------------------------+\n",
            "|    Customer_subtype|collect_set(Number_of_houses)|\n",
            "+--------------------+-----------------------------+\n",
            "|Large family, emp...|                       [1, 2]|\n",
            "|Religious elderly...|                       [1, 2]|\n",
            "|Large religious f...|                       [1, 2]|\n",
            "+--------------------+-----------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zenj_Jdv1oyn",
        "outputId": "3fcfc638-406a-4ce1-966d-a58fc20c2ee0"
      },
      "source": [
        "df.groupby('Customer_subtype').agg(F.collect_list(\"Number_of_houses\")).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------------------------------+\n",
            "|    Customer_subtype|collect_list(Number_of_houses)|\n",
            "+--------------------+------------------------------+\n",
            "|Large family, emp...|          [2, 1, 2, 1, 2, 1...|\n",
            "|Religious elderly...|          [1, 1, 1, 1, 1, 1...|\n",
            "|Large religious f...|          [2, 1, 1, 2, 1, 1...|\n",
            "+--------------------+------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_Dut0WU1t1B",
        "outputId": "24f989b5-e291-4186-9d82-99b55ed30863"
      },
      "source": [
        "df = df.withColumn('Constant',F.lit('finance'))\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|Constant|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+\n",
            "|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0| finance|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0| finance|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0| finance|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AILSakP02HW9",
        "outputId": "cbb4b361-2094-482e-b952-396c065cb203"
      },
      "source": [
        "'''\n",
        "User-Defined Functions (UDFs)\n",
        "\n",
        "'''\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "df.groupby(\"Avg_age\").count().show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----+\n",
            "|    Avg_age|count|\n",
            "+-----------+-----+\n",
            "|70-80 years|    8|\n",
            "|50-60 years|  373|\n",
            "|30-40 years|  496|\n",
            "+-----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzhYWrP22qgL"
      },
      "source": [
        "def age_category(age):\n",
        "    if age  == \"20-30 years\":\n",
        "        return \"Young\"\n",
        "    elif age== \"30-40 years\":\n",
        "        return \"Mid Aged\"\n",
        "    elif ((age== \"40-50 years\") or (age== \"50-60 years\")) :\n",
        "        return \"Old\"\n",
        "    else:\n",
        "        return \"Very Old\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxYyyW4_2y6w",
        "outputId": "9482cfa4-eeed-4a94-880f-d9567b0bb07a"
      },
      "source": [
        "age_udf = udf(age_category,StringType())\n",
        "df=df.withColumn('age_category',age_udf(df['Avg_age']))\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|Constant|age_category|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+\n",
            "|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0| finance|    Mid Aged|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0| finance|    Mid Aged|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0| finance|    Mid Aged|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwesb0lo3FKq"
      },
      "source": [
        "min_sal=1361\n",
        "max_sal=48919896\n",
        "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
        "\n",
        "def  scaled_salary(salary):\n",
        "    scaled_sal = ((salary-min_sal)/(max_sal-min_sal))\n",
        "    return scaled_sal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7lepy_33nYp",
        "outputId": "b6452764-0f42-4722-eb14-0a3611c9a2d2"
      },
      "source": [
        "scaling_udf = pandas_udf(scaled_salary,DoubleType())\n",
        "df.withColumn(\"Scaled_salary\",scaling_udf(df['Avg_Salary'])).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+--------------------+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|Constant|age_category|       Scaled_salary|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+--------------------+\n",
            "|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0| finance|    Mid Aged|8.901329526732557E-4|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0| finance|    Mid Aged| 7.40291997705982E-4|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0| finance|    Mid Aged| 5.42820834679534E-4|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SUfkhCp359M",
        "outputId": "c23249a6-b08b-4175-89fb-fc363c11f572"
      },
      "source": [
        "'''Joins'''\n",
        "region_data = spark.createDataFrame([('Family with grown ups','PN'),\n",
        "                     ('Driven Growers','GJ'),('Conservative families','DD'),('Cruising Seniors','DL'),('Average Family ','MN'),\n",
        "                     ('Living well','KA'),('Successful hedonists','JH'),('Retired and Religious','AX'),\n",
        "                     ('Career Loners','HY'),('Farmers','JH')],\n",
        "                     schema = StructType().add(\"Customer_main_type\",\"string\").add(\"Region Code\",\"string\"))\n",
        "\n",
        "region_data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+\n",
            "|  Customer_main_type|Region Code|\n",
            "+--------------------+-----------+\n",
            "|Family with grown...|         PN|\n",
            "|      Driven Growers|         GJ|\n",
            "|Conservative fami...|         DD|\n",
            "|    Cruising Seniors|         DL|\n",
            "|     Average Family |         MN|\n",
            "|         Living well|         KA|\n",
            "|Successful hedonists|         JH|\n",
            "|Retired and Relig...|         AX|\n",
            "|       Career Loners|         HY|\n",
            "|             Farmers|         JH|\n",
            "+--------------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqB_fvFN4jv5",
        "outputId": "5d700ce7-62c8-410d-943c-0d6b98b5dfe7"
      },
      "source": [
        "new_df = df.join(region_data,on='Customer_main_type')\n",
        "new_df.groupby(\"Region COde\").count().show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----+\n",
            "|Region COde|count|\n",
            "+-----------+-----+\n",
            "|         JH|  287|\n",
            "|         HY|   15|\n",
            "|         DD|  236|\n",
            "+-----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzg4yPqV5gSp",
        "outputId": "8cc2d7b2-f667-46a9-df19-c431841c4d17"
      },
      "source": [
        "df.groupBy('Customer_main_type').pivot('Avg_age').sum('Avg_Salary').fillna(0).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
            "|  Customer_main_type|20-30 years|30-40 years|40-50 years|50-60 years|60-70 years|70-80 years|\n",
            "+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
            "|             Farmers|          0|     462027|    2031235|     316206|          0|          0|\n",
            "|       Career Loners|     143998|     176639|      25701|     105193|      32558|          0|\n",
            "|Retired and Relig...|     126350|     336631|    2975266|    1687711|     335357|      61124|\n",
            "|Successful hedonists|      42261|  171278764| 1223362814| 1563071675|  200340129|      15518|\n",
            "|         Living well|     460528|    2965303|    1795405|     331304|          0|          0|\n",
            "+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_NJywnJ6ijV"
      },
      "source": [
        "#Window Functions\n",
        "\n",
        "\n",
        "    Aggregations\n",
        "\n",
        "    Ranking\n",
        "\n",
        "    Analytics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CXFobK26NBF",
        "outputId": "e259f09e-db30-423e-934a-2a8171663674"
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col,row_number\n",
        "win = Window.orderBy(df['Avg_Salary'].desc())\n",
        "df = df.withColumn('rank', row_number().over(win).alias('rank'))\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|Constant|age_category|rank|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "| High status seniors|               1|                 2|60-70 years|Successful hedonists|  48919896|    0| finance|    Very Old|   1|\n",
            "|High Income, expe...|               1|                 2|50-60 years|Successful hedonists|  48177970|    0| finance|         Old|   2|\n",
            "|High Income, expe...|               1|                 2|50-60 years|Successful hedonists|  48069548|    1| finance|         Old|   3|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P85UfjVO7BSp"
      },
      "source": [
        "win_1 = Window.partitionBy(\"Customer_subtype\").orderBy(df['Avg_Salary'].desc())\n",
        "df=df.withColumn('rank',row_number().over(win_1))#.alias('rank')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tpueO5D76eL",
        "outputId": "bd161389-644b-4599-bfec-3e535f94de81"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|Constant|age_category|rank|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "|Large family, emp...|               2|                 3|30-40 years|Family with grown...|     49418|    0| finance|    Mid Aged|   1|\n",
            "|Large family, emp...|               1|                 4|40-50 years|Family with grown...|     48390|    0| finance|         Old|   2|\n",
            "|Large family, emp...|               1|                 3|40-50 years|Family with grown...|     48272|    0| finance|         Old|   3|\n",
            "|Large family, emp...|               1|                 2|40-50 years|Family with grown...|     47684|    0| finance|         Old|   4|\n",
            "|Large family, emp...|               1|                 3|40-50 years|Family with grown...|     46703|    0| finance|         Old|   5|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ag6J36379bb",
        "outputId": "75f3e0c5-0ca1-4f80-b6ae-068ab7115889"
      },
      "source": [
        "df.groupBy('rank').count().orderBy('rank').show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|rank|count|\n",
            "+----+-----+\n",
            "|   1|   39|\n",
            "|   2|   37|\n",
            "|   3|   36|\n",
            "+----+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTjERYOh8IQM",
        "outputId": "69f75abe-fb34-4508-a596-13406b225f34"
      },
      "source": [
        "df.filter(col('rank')<4).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|Constant|age_category|rank|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "|Large family, emp...|               2|                 3|30-40 years|Family with grown...|     49418|    0| finance|    Mid Aged|   1|\n",
            "|Large family, emp...|               1|                 4|40-50 years|Family with grown...|     48390|    0| finance|         Old|   2|\n",
            "|Large family, emp...|               1|                 3|40-50 years|Family with grown...|     48272|    0| finance|         Old|   3|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+--------+------------+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhTYywejtotD"
      },
      "source": [
        "#Spark Structured Streaming\n",
        "\n",
        "Spark Streaming then, Spark structured streaming now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRjfaY7a8SKd"
      },
      "source": [
        "spark_stream = SparkSession.builder.appName('StructuredStreaming').getOrCreate()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx196G7l9-s8"
      },
      "source": [
        "df_1 = spark_stream.createDataFrame([(\"XN203\",'FB',300,30),(\"XN201\",'Twitter',10,19),(\"XN202\",'Insta',500,45)],[\"user_id\",\"app\",\"time_in_secs\",\"age\"]).\\\n",
        "write.csv('/content/csv_folder',mode='append')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5DHn9nk9-vc"
      },
      "source": [
        "schema=StructType().add(\"user_id\",\"string\").add(\"app\",\"string\").add(\"time_in_secs\", \"integer\").add(\"age\", \"integer\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmbuMLUB9-xP"
      },
      "source": [
        "data = spark.readStream.option(\"sep\",\",\").schema(schema).csv('/content/csv_folder')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYYMQcdl9-0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0ab613-4a69-4c7a-f98a-f51fa980f084"
      },
      "source": [
        "data.printSchema()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- app: string (nullable = true)\n",
            " |-- time_in_secs: integer (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNP2U89c9-5z"
      },
      "source": [
        "app_count = data.groupBy('app').count()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBJopnxH9-8x"
      },
      "source": [
        "query = (app_count.writeStream.queryName('count_query').outputMode('complete').format('memory').start())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cnhnR-j9--v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "260b721f-980c-4349-d61e-cdf14ea93997"
      },
      "source": [
        "spark.sql(\"select * from count_query\").toPandas().head(3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Insta</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FB</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Twitter</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       app  count\n",
              "0    Insta      1\n",
              "1       FB      1\n",
              "2  Twitter      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arRQQT3w9_BE"
      },
      "source": [
        "fb_data = data.filter(data['app']=='FB')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9FFWBk89_DA"
      },
      "source": [
        "fb_avg_time = fb_data.groupby('user_id').agg(F.avg('time_in_secs'))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c-ZVPpi9_Fv"
      },
      "source": [
        "fb_query = (fb_avg_time.writeStream.queryName('fb_query').outputMode('complete').format('memory').start())"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbAPqk9f9_IY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "32752716-863e-4b00-d7fa-df3a7ee9e680"
      },
      "source": [
        "spark.sql(\"select * from fb_query\").toPandas().head(3)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>avg(time_in_secs)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XN203</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  user_id  avg(time_in_secs)\n",
              "0   XN203              300.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e263dsf09_Kt"
      },
      "source": [
        "df_3=spark.createDataFrame([(\"XN20\",'FB',100,30),(\"XN2044\",'FB',10,19),(\"XN2102\",'FB',2000,45)],[\"user_id\",\"app\",\"time_in_secs\",\"age\"]).write.csv(\"/content/csv_folder\",mode='append')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urjO02Mb9_NE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ced00949-a1cd-4782-b5d6-0640c0a5c891"
      },
      "source": [
        "spark.sql(\"select * from fb_query \").toPandas().head(5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>avg(time_in_secs)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XN203</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XN20</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XN201</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>XN2044</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XN202</td>\n",
              "      <td>2000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  user_id  avg(time_in_secs)\n",
              "0   XN203              150.0\n",
              "1    XN20              100.0\n",
              "2   XN201               10.0\n",
              "3  XN2044               10.0\n",
              "4   XN202             2000.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIwsMDBv9_QY"
      },
      "source": [
        "app_df = data.groupBy('app').agg(F.sum('time_in_secs').alias('total_time')).orderBy('total_time',ascending=False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqcU-8Oz9_R_"
      },
      "source": [
        "app_query = (app_df.writeStream.queryName('app_wise_query').outputMode('Complete').format('memory').start())\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdQeFdgB9_U4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "3f77dcfc-f1bf-4578-f2c9-3472ece39739"
      },
      "source": [
        "spark.sql(\"select * from app_wise_query\").toPandas().head(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app</th>\n",
              "      <th>total_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FB</td>\n",
              "      <td>10850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Insta</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Twitter</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       app  total_time\n",
              "0       FB       10850\n",
              "1    Insta         500\n",
              "2  Twitter          10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go6P8CgC9_XL"
      },
      "source": [
        "df_5=spark.createDataFrame([(\"XN203\",'FB',1000,30),(\"XN201\",'Insta',400,19),(\"XN202\",'Twitter',900,45)],[\"user_id\",\"app\",\"time_in_secs\",\"age\"]).write.csv(\"csv_folder\",mode='append')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qARlh9TI9_Zl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "d2106f8b-1013-4125-d919-e4a0f83f237e"
      },
      "source": [
        "spark.sql(\"select * from app_wise_query\").toPandas().head(5)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app</th>\n",
              "      <th>total_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FB</td>\n",
              "      <td>11350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Insta</td>\n",
              "      <td>530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Twitter</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       app  total_time\n",
              "0       FB       11350\n",
              "1    Insta         530\n",
              "2  Twitter         110"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtsWbjpj9_ca"
      },
      "source": [
        "app_df=spark.createDataFrame([('FB','FACEBOOK'),('Insta','INSTAGRAM'),('Twitter','TWITTER')],[\"app\", \"full_name\"])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVhwV3UN9_fC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a0950f-f66c-446b-d820-3405c9aae2b8"
      },
      "source": [
        "app_df.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+\n",
            "|    app|full_name|\n",
            "+-------+---------+\n",
            "|     FB| FACEBOOK|\n",
            "|  Insta|INSTAGRAM|\n",
            "|Twitter|  TWITTER|\n",
            "+-------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH8S4khb9_hs"
      },
      "source": [
        "app_stream_df = data.join(app_df,'app')\n",
        "join_query = (app_stream_df.writeStream.queryName('JoinQuery').outputMode('append').format('memory').start())"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EUh5xfb9_kA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7fabe897-6016-4a45-d58f-e7a1b041c210"
      },
      "source": [
        "spark.sql(\"select * from JoinQuery\").toPandas().head(3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app</th>\n",
              "      <th>user_id</th>\n",
              "      <th>time_in_secs</th>\n",
              "      <th>age</th>\n",
              "      <th>full_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FB</td>\n",
              "      <td>XN20</td>\n",
              "      <td>100</td>\n",
              "      <td>30</td>\n",
              "      <td>FACEBOOK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FB</td>\n",
              "      <td>XN20</td>\n",
              "      <td>100</td>\n",
              "      <td>30</td>\n",
              "      <td>FACEBOOK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FB</td>\n",
              "      <td>XN203</td>\n",
              "      <td>300</td>\n",
              "      <td>30</td>\n",
              "      <td>FACEBOOK</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  app user_id  time_in_secs  age full_name\n",
              "0  FB    XN20           100   30  FACEBOOK\n",
              "1  FB    XN20           100   30  FACEBOOK\n",
              "2  FB   XN203           300   30  FACEBOOK"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkv8SeM8f8VX"
      },
      "source": [
        "#MLLIB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzKiAfWm9_og"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unI-5hpp9_rX"
      },
      "source": [
        "spark = SparkSession.builder.appName('BasicStats').getOrCreate()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w-oo1o29_tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23bb21d5-8077-4eda-a712-a14310e96979"
      },
      "source": [
        "!sudo apt install docker-ce"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package docker-ce is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "E: Package 'docker-ce' has no installation candidate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otrQbN02g1xC"
      },
      "source": [
        "df = spark.read.csv('/content/learn-pyspark/chap_2/customer_data.csv',header=True,inferSchema=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW7elmxziQfH",
        "outputId": "4cebdf0b-249e-44eb-a81c-eac8295a02dc"
      },
      "source": [
        "df.show(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XIcEZs5iR-R",
        "outputId": "a5ef9a00-54a4-498c-99f5-f7a4b5c0b115"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC-d7Vjhixp4"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['Avg_Salary','Avg_size_household'], outputCol='features')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGWOhw3cjddZ"
      },
      "source": [
        "df_n = df.select(['Avg_Salary','Avg_size_household'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGVl0_skjkXS",
        "outputId": "61317e21-34db-44fc-df9e-180b1a5688f2"
      },
      "source": [
        "df_n.show(3)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------------+\n",
            "|Avg_Salary|Avg_size_household|\n",
            "+----------+------------------+\n",
            "|     44905|                 3|\n",
            "|     37575|                 2|\n",
            "|     27915|                 2|\n",
            "+----------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NokbUzfajRsR"
      },
      "source": [
        "df_new = assembler.transform(df_n)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmfmnT6CjoxH",
        "outputId": "f0738be5-d5b6-4b76-abb8-aa1999408efd"
      },
      "source": [
        "df_new.show(3)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------------+-------------+\n",
            "|Avg_Salary|Avg_size_household|     features|\n",
            "+----------+------------------+-------------+\n",
            "|     44905|                 3|[44905.0,3.0]|\n",
            "|     37575|                 2|[37575.0,2.0]|\n",
            "|     27915|                 2|[27915.0,2.0]|\n",
            "+----------+------------------+-------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVqSXsv-jrh_"
      },
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "pearson_corr = Correlation.corr(df_new,'features')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIXCwGj8kGNm",
        "outputId": "30bf52a5-ca8a-40de-e102-d1e0fde72633"
      },
      "source": [
        "pearson_corr.show(2,False)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------+\n",
            "|pearson(features)                                                                        |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "|1.0                   -0.06585562673466173  \n",
            "-0.06585562673466173  1.0                   |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvQAj335kJvk",
        "outputId": "d627038f-2a78-4e72-dfdb-b231d830a094"
      },
      "source": [
        "spearman_corr = Correlation.corr(df_new,'features','spearman')\n",
        "spearman_corr.show(2,False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------+\n",
            "|spearman(features)                                                                           |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "|1.0                    -0.016275912045928723  \n",
            "-0.016275912045928723  1.0                    |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmURD9X4khK8"
      },
      "source": [
        "'''\n",
        "Chi Square Test\n",
        "'''\n",
        "\n",
        "df=spark.read.csv('chi_sq.csv',inferSchema=True,header=True)#smoker vs non smoker\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "marital_indexer = StringIndexer(inputCol=\"marital\", outputCol=\"marital_num\").fit(df)\n",
        "df = marital_indexer.transform(df)\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "marital_encoder = OneHotEncoder(inputCol=\"marital_num\", outputCol=\"marital_vector\")\n",
        "df = marital_encoder.transform(df)\n",
        "housing_indexer = StringIndexer(inputCol=\"housing\", outputCol=\"housing_num\").fit(df)\n",
        "df = housing_indexer.transform(df)\n",
        "housing_encoder = OneHotEncoder(inputCol=\"housing_num\", outputCol=\"housing_vector\")\n",
        "df = housing_encoder.transform(df)\n",
        "df_assembler = VectorAssembler(inputCols=['marital_vector','housing_vector'], outputCol=\"features\")\n",
        "df = df_assembler.transform(df)\n",
        "chi_sq = ChiSquareTest.test(df, \"features\", \"label\").head()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5KpObrCr2h-",
        "outputId": "0b0a9614-b277-444f-cb71-40da5f81baa5"
      },
      "source": [
        "df.show(3)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|    Customer_subtype|Number_of_houses|Avg_size_household|    Avg_age|  Customer_main_type|Avg_Salary|label|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "|Lower class large...|               1|                 3|30-40 years|Family with grown...|     44905|    0|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     37575|    0|\n",
            "|Mixed small town ...|               1|                 2|30-40 years|Family with grown...|     27915|    0|\n",
            "+--------------------+----------------+------------------+-----------+--------------------+----------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQbv9wClk-hY"
      },
      "source": [
        "#Transformations\n",
        "\n",
        "#Binarizer\n",
        "from pyspark.ml.feature import Binarizer\n",
        "binarizer = Binarizer(threshold=2,inputCol='Number_of_houses',outputCol='binarized_Number_of_houses')\n",
        "new_df = binarizer.transform(df)\n",
        "new_df.show(3)\n",
        "\n",
        "#PCA\n",
        "from pyspark.ml.feature import PCA\n",
        "assembler = VectorAssembler(inputCols=[col for col in df.columns if col != 'label'],outputCol=\"features\")\n",
        "df_new = assembler.transform(df)\n",
        "pca = PCA(k=2,inputCol=\"features\",outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(df_new)\n",
        "pca_comp = pca_model.transform(df_new).select('pca_features')\n",
        "pca_comp.show(truncate=False)\n",
        "\n",
        "#Normalizer\n",
        "from pysparl.ml.featrure import Normalizer\n",
        "normalizer = Normalizer(inputCol=\"features\",outputCol=\"norm_features\",p=1.0)\n",
        "normalized_l1_data = normalizer.transform(df_new)\n",
        "normalized_l1_data.select('norm_features').show(truncate=False)\n",
        "\n",
        "#Standard Scaling\n",
        "[In]: from pyspark.ml.feature import StandardScaler\n",
        "[In]: scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
        "                             withStd=False, withMean=True)\n",
        "[In]: scaler_model = scaler.fit(df_new)\n",
        "[In]: scaled_data = scaler_model.transform(df_new)\n",
        "[In]: scaled_data.select('scaled_features').show(truncate=False)\n",
        "\n",
        "#Min-Max Scaling\n",
        "[In]: from pyspark.ml.feature import MinMaxScaler\n",
        "[In]: mm_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"mm_scaled_features\")\n",
        "[In]: mm_scaler_model = mm_scaler.fit(df_new)\n",
        "[In]: rescaled_df = mm_scaler_model.transform(df_new)\n",
        "[In]: rescaled_df.select(\"features\", \"mm_scaled_features\").show()\n",
        "\n",
        "#MaxAbsScaler\n",
        "[In]: from pyspark.ml.feature import MaxAbsScaler\n",
        "[In]: mxabs_scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"mxabs_features\")\n",
        "[In]: mxabs_scaler_model = mxabs_scaler.fit(df_new)\n",
        "[In]: rescaled_df = mxabs_scaler_model.transform(df_new)\n",
        "[In]: rescaled_df.select(\"features\", \"mxabs_features\").show()\n",
        "\n",
        "#Binning\n",
        "[In]: from pyspark.ml.feature import Bucketizer\n",
        "[In]: splits = [0.0,1.0,2.0,3.0,4.0,5.0,float(\"inf\")]\n",
        "[In]: bucketizer = Bucketizer(splits=splits, inputCol=\"label\", outputCol=\"label_bins\")\n",
        "[In]: binned_df = bucketizer.transform(df)\n",
        "[In]: binned_df.select(['label','label_bins']).show(10,False)\n",
        "[In]: binned_df.groupBy('label_bins').count().show()\n",
        "print(bucketizer.getSplits())-1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzZ1tv6kyBZ4"
      },
      "source": [
        "#Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGpeWWRwrEN9"
      },
      "source": [
        "#Step 1: Load the Dataset\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Classification').getOrCreate()\n",
        "df = spark.read.csv('/content/learn-pyspark/chap_5/classification_data.csv',inferSchema=True,\n",
        "                    header=True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVZdFsDqyi_8",
        "outputId": "6732576b-33e3-41d2-823c-c027bb74bb2a"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46751"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f000Gv60yktP",
        "outputId": "db0449cf-1e88-4f16-cc82-cb2ae3bf219f"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- loan_id: string (nullable = true)\n",
            " |-- loan_purpose: string (nullable = true)\n",
            " |-- is_first_loan: integer (nullable = true)\n",
            " |-- total_credit_card_limit: integer (nullable = true)\n",
            " |-- avg_percentage_credit_card_limit_used_last_year: double (nullable = true)\n",
            " |-- saving_amount: integer (nullable = true)\n",
            " |-- checking_amount: integer (nullable = true)\n",
            " |-- is_employed: integer (nullable = true)\n",
            " |-- yearly_salary: integer (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- dependent_number: integer (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-_bEyn7yu0C",
        "outputId": "1510731b-7582-4ba9-880d-0c19fdabe3b1"
      },
      "source": [
        "#Step 2: Explore the Dataframe\n",
        "df.show(3)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+-------------+-----------------------+-----------------------------------------------+-------------+---------------+-----------+-------------+---+----------------+-----+\n",
            "|loan_id|loan_purpose|is_first_loan|total_credit_card_limit|avg_percentage_credit_card_limit_used_last_year|saving_amount|checking_amount|is_employed|yearly_salary|age|dependent_number|label|\n",
            "+-------+------------+-------------+-----------------------+-----------------------------------------------+-------------+---------------+-----------+-------------+---+----------------+-----+\n",
            "|    A_1|    personal|            1|                   7900|                                            0.8|         1103|           6393|          1|        16400| 42|               4|    0|\n",
            "|    A_2|    personal|            0|                   3300|                                           0.29|         2588|            832|          1|        75500| 56|               1|    0|\n",
            "|    A_3|    personal|            0|                   7600|                                            0.9|         1651|           8868|          1|        59000| 46|               1|    0|\n",
            "+-------+------------+-------------+-----------------------+-----------------------------------------------+-------------+---------------+-----------+-------------+---+----------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFTWTmHmzf0u",
        "outputId": "beedba0e-7c12-4219-8efa-c1e50e1bfb69"
      },
      "source": [
        "df.groupBy('label').count().alias('label_group').show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1|16201|\n",
            "|    0|30550|\n",
            "+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9CEoFGDzryb",
        "outputId": "bfe46832-e236-4474-f3ca-05d4ca2c4936"
      },
      "source": [
        "df.groupBy('loan_purpose').count().show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "|loan_purpose|count|\n",
            "+------------+-----+\n",
            "|      others| 6763|\n",
            "|   emergency| 7562|\n",
            "|    property|11388|\n",
            "|  operations|10580|\n",
            "|    personal|10458|\n",
            "+------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2FfhXyF0Dqp"
      },
      "source": [
        "#Step 3: Data Transformation\n",
        "\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer,VectorAssembler\n",
        "\n",
        "loan_purpose_indexer = StringIndexer(inputCol='loan_purpose',outputCol='loan_purpose_index').fit(df)\n",
        "df = loan_purpose_indexer.transform(df)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i3_ysY-6jt3"
      },
      "source": [
        "loan_encoder = OneHotEncoder(inputCol=\"loan_purpose_index\",outputCol=\"loan_purpose_vector\").fit(df)\n",
        "df = loan_encoder.transform(df)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-3CaQ9r7l4I",
        "outputId": "325b1d61-b207-4bb7-9a93-3146e757088c"
      },
      "source": [
        "df.show(3)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+-------------+-----------------------+-----------------------------------------------+-------------+---------------+-----------+-------------+---+----------------+-----+------------------+-------------------+\n",
            "|loan_id|loan_purpose|is_first_loan|total_credit_card_limit|avg_percentage_credit_card_limit_used_last_year|saving_amount|checking_amount|is_employed|yearly_salary|age|dependent_number|label|loan_purpose_index|loan_purpose_vector|\n",
            "+-------+------------+-------------+-----------------------+-----------------------------------------------+-------------+---------------+-----------+-------------+---+----------------+-----+------------------+-------------------+\n",
            "|    A_1|    personal|            1|                   7900|                                            0.8|         1103|           6393|          1|        16400| 42|               4|    0|               2.0|      (4,[2],[1.0])|\n",
            "|    A_2|    personal|            0|                   3300|                                           0.29|         2588|            832|          1|        75500| 56|               1|    0|               2.0|      (4,[2],[1.0])|\n",
            "|    A_3|    personal|            0|                   7600|                                            0.9|         1651|           8868|          1|        59000| 46|               1|    0|               2.0|      (4,[2],[1.0])|\n",
            "+-------+------------+-------------+-----------------------+-----------------------------------------------+-------------+---------------+-----------+-------------+---+----------------+-----+------------------+-------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50oZ-qq96lT-"
      },
      "source": [
        "df_vec_assembler = VectorAssembler(inputCols=['is_first_loan',\n",
        " 'total_credit_card_limit',\n",
        " 'avg_percentage_credit_card_limit_used_last_year',\n",
        " 'saving_amount',\n",
        " 'checking_amount',\n",
        " 'is_employed',\n",
        " 'yearly_salary',\n",
        " 'age',\n",
        " 'dependent_number',\n",
        " 'loan_purpose_vector'], outputCol=\"features\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA8y-yNM71IM",
        "outputId": "5b6779e7-344e-454b-d059-c93f335bcff5"
      },
      "source": [
        "df = df_vec_assembler.transform(df)\n",
        "df.select(['features','label']).show(10,False)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------+-----+\n",
            "|features                                                            |label|\n",
            "+--------------------------------------------------------------------+-----+\n",
            "|[1.0,7900.0,0.8,1103.0,6393.0,1.0,16400.0,42.0,4.0,0.0,0.0,1.0,0.0] |0    |\n",
            "|[0.0,3300.0,0.29,2588.0,832.0,1.0,75500.0,56.0,1.0,0.0,0.0,1.0,0.0] |0    |\n",
            "|[0.0,7600.0,0.9,1651.0,8868.0,1.0,59000.0,46.0,1.0,0.0,0.0,1.0,0.0] |0    |\n",
            "|[1.0,3400.0,0.38,1269.0,6863.0,1.0,26000.0,55.0,8.0,0.0,0.0,1.0,0.0]|0    |\n",
            "|[0.0,2600.0,0.89,1310.0,3423.0,1.0,9700.0,41.0,4.0,0.0,0.0,0.0,1.0] |1    |\n",
            "|[0.0,7600.0,0.51,1040.0,2406.0,1.0,22900.0,52.0,0.0,0.0,1.0,0.0,0.0]|0    |\n",
            "|[1.0,6900.0,0.82,2408.0,5556.0,1.0,34800.0,48.0,4.0,0.0,1.0,0.0,0.0]|0    |\n",
            "|[0.0,5700.0,0.56,1933.0,4139.0,1.0,32500.0,64.0,2.0,0.0,0.0,1.0,0.0]|0    |\n",
            "|[1.0,3400.0,0.95,3866.0,4131.0,1.0,13300.0,23.0,3.0,0.0,0.0,1.0,0.0]|0    |\n",
            "|[0.0,2900.0,0.91,88.0,2725.0,1.0,21100.0,52.0,1.0,0.0,0.0,1.0,0.0]  |1    |\n",
            "+--------------------------------------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1xouppG8EsR"
      },
      "source": [
        "model_df = df.select(['features','label'])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kO-0E638Snw"
      },
      "source": [
        "#Step 4: Splitting into Train and Test Data\n",
        "train_df,test_df = model_df.randomSplit([0.70,0.30])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBj2tw5H8c3j",
        "outputId": "bb4421a7-6f46-4154-f57d-b2b63946fb58"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "log_reg = LogisticRegression().fit(train_df)\n",
        "lr_summary = log_reg.summary\n",
        "lr_summary.accuracy"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8933528122717312"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKqCGMvW8v8U",
        "outputId": "02861895-6c47-44db-fe42-0d9928d8f97b"
      },
      "source": [
        "lr_summary.areaUnderROC"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9590400974642718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHfnNAcP9_OE",
        "outputId": "044cd888-d853-423e-f4af-bd54c50e71f8"
      },
      "source": [
        "print(lr_summary.precisionByLabel)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9229937473555545, 0.838929650410013]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyb_1d3Y-DV5",
        "outputId": "132c2457-6ec6-42be-b974-4b538d4ae3bd"
      },
      "source": [
        "print(lr_summary.recallByLabel)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9132052653611796, 0.855771770714097]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX8g-m4h-EiT"
      },
      "source": [
        "predictions = log_reg.transform(test_df)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVpZCd7y-Gwg",
        "outputId": "1d0ab19a-fbef-408b-c7c7-29d7d106a7bf"
      },
      "source": [
        "predictions.show(10) "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|            features|label|       rawPrediction|         probability|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|(13,[0,1,2,3,4,7]...|    1|[-6.8525548483602...|[0.00105563685221...|       1.0|\n",
            "|(13,[0,1,2,3,4,7]...|    0|[1.06037808660394...|[0.74276279151388...|       0.0|\n",
            "|(13,[0,1,2,3,4,7]...|    1|[-2.2167418772478...|[0.09825710497559...|       1.0|\n",
            "|(13,[0,1,2,3,4,7]...|    0|[6.69302569032580...|[0.99876200785470...|       0.0|\n",
            "|(13,[0,1,2,3,4,7]...|    1|[1.26395396565278...|[0.77970600995358...|       0.0|\n",
            "|(13,[0,1,2,3,4,7,...|    1|[-6.5102931218942...|[0.00148583249764...|       1.0|\n",
            "|(13,[0,1,2,3,4,7,...|    1|[-5.6831282363765...|[0.00339135627548...|       1.0|\n",
            "|(13,[0,1,2,3,4,7,...|    1|[-5.0823029621792...|[0.00616732929554...|       1.0|\n",
            "|(13,[0,1,2,3,4,7,...|    1|[-6.3285962618067...|[0.00178135813495...|       1.0|\n",
            "|(13,[0,1,2,3,4,7,...|    1|[-5.9117058421392...|[0.00270025315958...|       1.0|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxuw5rPp-IHJ"
      },
      "source": [
        "[In]: model_predictions = log_reg.transform(test_df)\n",
        "[In]: model_predictions = log_reg.evaluate(test_df)\n",
        "[In]: model_predictions.accuracy\n",
        "[Out]: 0.8945984906300347\n",
        "[In]: model_predictions.areaUnderROC\n",
        "[Out]: 0.9594316478468224\n",
        "[In]: print(model_predictions.recallByLabel)\n",
        "[Out]: [0.9129581151832461, 0.8608235010835541]\n",
        "[In]: print(model_predictions.precisionByLabel)\n",
        "[Out]: [0.9234741162452006, 0.8431603773584906] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02Z6xJ22-QZ8"
      },
      "source": [
        "#Step 6: Hyperparameter Tuning\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf_model = rf.fit(train_df)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7JJzHj_-l_W"
      },
      "source": [
        "model_predictions = rf_model.transform(test_df)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKndEGiX-xMZ"
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "rf = RandomForestClassifier()\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [5,10,20,25,30])\n",
        "             .addGrid(rf.maxBins, [20,30,40 ])\n",
        "             .addGrid(rf.numTrees, [5, 20,50])\n",
        "             .build())\n",
        "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "cv_model = cv.fit(train_df)\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr98IYcS_IEb",
        "outputId": "abf28e6a-d0c7-4d31-d225-4a501de3ce32"
      },
      "source": [
        "best_rf_model = cv_model.bestModel\n",
        "model_predictions = best_rf_model.transform(test_df)\n",
        "model_predictions = best_rf_model.transform(test_df)\n",
        "true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "recall_rate=float(true_pos)/(actual_pos)\n",
        "print(recall_rate)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9035920726672172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X30xWE_9_t__"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVi0gKHfQUeO"
      },
      "source": [
        "#Regression\n",
        "\n",
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdNfGTcYQXnj",
        "outputId": "251d0215-4eb6-449d-e83b-fdb29a56cc44"
      },
      "source": [
        "#Step 1: Create the Spark Session Object\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Supervised').getOrCreate()\n",
        "df=spark.read.csv('/content/learn-pyspark/chap_6/Linear_regression_dataset.csv',inferSchema=True,header=True)\n",
        "df.show(3)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+-----+-----+-----+-----+\n",
            "|var_1|var_2|var_3|var_4|var_5|label|\n",
            "+-----+-----+-----+-----+-----+-----+\n",
            "|  734|  688|   81|0.328|0.259|0.418|\n",
            "|  700|  600|   94| 0.32|0.247|0.389|\n",
            "|  712|  705|   93|0.311|0.247|0.417|\n",
            "+-----+-----+-----+-----+-----+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH4uiCwaQ5kx",
        "outputId": "3671521b-30ce-4fd6-f789-66418555cd1a"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'label']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-gzB5g0RBLz"
      },
      "source": [
        "from pyspark.ml.linalg import Vector\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "vec_assembler = VectorAssembler(inputCols=['var_1', 'var_2', 'var_3', 'var_4', 'var_5'],outputCol='features')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV5IRyFiRtJk"
      },
      "source": [
        "features_df = vec_assembler.transform(df)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA3lD9MvSKaj",
        "outputId": "4d4d2185-4fc1-478c-bf52-2460e084f92f"
      },
      "source": [
        "features_df.show(3)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+-----+-----+-----+-----+--------------------+\n",
            "|var_1|var_2|var_3|var_4|var_5|label|            features|\n",
            "+-----+-----+-----+-----+-----+-----+--------------------+\n",
            "|  734|  688|   81|0.328|0.259|0.418|[734.0,688.0,81.0...|\n",
            "|  700|  600|   94| 0.32|0.247|0.389|[700.0,600.0,94.0...|\n",
            "|  712|  705|   93|0.311|0.247|0.417|[712.0,705.0,93.0...|\n",
            "+-----+-----+-----+-----+-----+-----+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe-VW47vRyBg"
      },
      "source": [
        "model_df = features_df.select(['features','label'])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUveOXoZSSaF"
      },
      "source": [
        "train,test = model_df.randomSplit([.80,.20])"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kASSnMASZeJ"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWd5bsWUStea",
        "outputId": "80bf6c38-648d-45af-d93d-262cbe35d233"
      },
      "source": [
        "lr_model = lr.fit(train)\n",
        "predictions = lr_model.transform(test)\n",
        "predictions.show(3)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+-------------------+\n",
            "|            features|label|         prediction|\n",
            "+--------------------+-----+-------------------+\n",
            "|[501.0,774.0,51.0...|0.315|0.32941576721355015|\n",
            "|[524.0,665.0,65.0...|0.336|0.33526828055190494|\n",
            "|[527.0,569.0,75.0...|0.341|0.33404522272680504|\n",
            "+--------------------+-----+-------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQexaQHvS8A1",
        "outputId": "af23f8d2-321c-4af8-c733-22d8a892d8be"
      },
      "source": [
        "model_evaluate = lr_model.evaluate(test)\n",
        "print(model_evaluate.r2)\n",
        "print(model_evaluate.meanSquaredError)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8665399365021725\n",
            "0.00014751436252392696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjtOjTCkT7ip"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "glr = GeneralizedLinearRegression()\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwvXOyCxU5mV",
        "outputId": "b1570ee1-2f97-483b-e69a-8a0e980a51d2"
      },
      "source": [
        "glr_model = glr.fit(train)\n",
        "glr_model.summary"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Coefficients:\n",
              "    Feature Estimate Std Error T Value P Value\n",
              "(Intercept)   0.1817    0.0158 11.5292  0.0000\n",
              "      var_1   0.0003    0.0000 23.8724  0.0000\n",
              "      var_2   0.0001    0.0000  5.2442  0.0000\n",
              "      var_3   0.0002    0.0001  2.4339  0.0151\n",
              "      var_4  -0.6537    0.0672 -9.7327  0.0000\n",
              "      var_5   0.5017    0.0597  8.4053  0.0000\n",
              "\n",
              "(Dispersion parameter for gaussian family taken to be 0.0001)\n",
              "    Null deviance: 1.0777 on 969 degrees of freedom\n",
              "Residual deviance: 0.1404 on 969 degrees of freedom\n",
              "AIC: -5843.7712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCfYo_2XVoX6",
        "outputId": "14ffd648-4867-40e4-e766-b77d0d1d1d24"
      },
      "source": [
        "model_prediction = glr_model.evaluate(test)\n",
        "model_prediction.predictions.show()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+-------------------+\n",
            "|            features|label|         prediction|\n",
            "+--------------------+-----+-------------------+\n",
            "|[501.0,774.0,51.0...|0.315|0.32941576721355015|\n",
            "|[524.0,665.0,65.0...|0.336|0.33526828055190494|\n",
            "|[527.0,569.0,75.0...|0.341|0.33404522272680504|\n",
            "|[537.0,660.0,63.0...|0.326|0.32878148867545504|\n",
            "|[550.0,631.0,76.0...|0.318| 0.3376626624781536|\n",
            "|[558.0,740.0,60.0...| 0.36|0.34866277732553574|\n",
            "|[564.0,648.0,74.0...|0.337|0.35119608054214957|\n",
            "|[567.0,587.0,84.0...|0.349|0.34744221279909987|\n",
            "|[568.0,708.0,57.0...|0.347| 0.3464119284930939|\n",
            "|[569.0,776.0,53.0...|0.348| 0.3590516826571347|\n",
            "|[570.0,655.0,66.0...| 0.34|0.34562368436206414|\n",
            "|[570.0,662.0,73.0...|0.337| 0.3488342303101433|\n",
            "|[573.0,634.0,75.0...|0.342|0.34848477303445524|\n",
            "|[574.0,586.0,81.0...| 0.36|0.35112672117515176|\n",
            "|[575.0,680.0,68.0...|0.344| 0.3538959828401379|\n",
            "|[575.0,864.0,55.0...|0.379| 0.3650182165612126|\n",
            "|[579.0,497.0,91.0...|0.352|0.33939234105473914|\n",
            "|[583.0,794.0,55.0...|0.371|0.35629984863661857|\n",
            "|[587.0,866.0,57.0...|0.373|0.36398508191765266|\n",
            "|[588.0,550.0,83.0...|0.351| 0.3458206272638318|\n",
            "+--------------------+-----+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thv40Ae6VuJM"
      },
      "source": [
        "\n",
        "[In]: glr = GeneralizedLinearRegression(family='Binomial')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.aic\n",
        "[Out]: 336.991\n",
        "[In]: glr = GeneralizedLinearRegression(family='Poisson')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: predictions=glr_model.evaluate(test)\n",
        "[In]: predictions.aic\n",
        "[Out]: 266.53\n",
        "[In]: glr = GeneralizedLinearRegression(family='Gamma')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]:model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.aic\n",
        "[Out]: -1903.81\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GghFRTvFX6cj",
        "outputId": "547a1315-fd96-42cd-bd94-a8691735194e"
      },
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "d_tree = DecisionTreeRegressor()\n",
        "d_tree_model = d_tree.fit(train)\n",
        "model_pred = d_tree_model.transform(test)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "dt_evaluator = RegressionEvaluator(metricName=\"r2\")\n",
        "dt_r2 = dt_evaluator.evaluate(model_pred)\n",
        "print(f'The r-square value of DecisionTreeRegressor is {dt_r2}')\n",
        "dt_evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
        "dt_r3 = dt_evaluator.evaluate(model_pred)\n",
        "print(f'The MSE value of DecisionTreeRegressor is {dt_r3}')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The r-square value of DecisionTreeRegressor is 0.8105437747921171\n",
            "The MSE value of DecisionTreeRegressor is 0.014470912910963866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFGKSLJVX6ew"
      },
      "source": [
        "\n",
        "[In]: from pyspark.ml.regression import RandomForestRegressor\n",
        "[In]: rf = RandomForestRegressor()\n",
        "[In]: rf_model = rf.fit(train)\n",
        "[In]:  rf_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.4395, 1: 0.045, 2: 0.0243, 3: 0.2725, 4: 0.2188})\n",
        "[In]: rf_model.getNumTrees\n",
        "[Out]: 20\n",
        "[In]: model_predictions = rf_model.transform(test)\n",
        "[In]: model_predictions.show()\n",
        "[Out]:\n",
        "[In]:rf_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: rf_r2 = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of RandomForestRegressor is {rf_r2}')\n",
        "[Out]: The r-square value of RandomForestRegressor is 0.8215863293044671\n",
        "[In]: rf_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: rf_rmse = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of RandomForestRegressor is {rf_rmse}')\n",
        "[Out]: The rmse value of RandomForestRegressor is 0.01365275410722947\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgXSxpi3X6hm"
      },
      "source": [
        "[In]: from pyspark.ml.regression import GBTRegressor\n",
        "[In]: gbt = GBTRegressor()\n",
        "[In]: gbt_model=gbt.fit(train)\n",
        "[In]: gbt_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.2325, 1: 0.2011, 2: 0.1645, 3: 0.2268, 4: 0.1751})\n",
        "[In]: model_predictions = gbt_model.transform(test)\n",
        "[In]: model_predictions.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD6x_RiPX6mW"
      },
      "source": [
        "\n",
        "[In]: gbt_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: gbt_r2 = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of GradientBoostedRegressor is {gbt_r2}')\n",
        "[Out]: The r-square value of GradientBoostedRegressor is 0.8477273892307596\n",
        "[In]: gbt_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: gbt_rmse = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of GradientBoostedRegressor is {gbt_rmse}')\n",
        "[Out]: The rmse value of GradientBoostedRegressor is 0.013305445803592103\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs082XTaX6oc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ShP0Tx0pNhk"
      },
      "source": [
        "#Building Multiple Models for Binary Classification Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tR_0TP3pKQm"
      },
      "source": [
        "spark = SparkSession.builder.appName('Classification').getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmIbpGsppiac"
      },
      "source": [
        "df=spark.read.csv('/content/learn-pyspark/chap_6/bank_data.csv',inferSchema=True,header=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-86Ptk9XGapb",
        "outputId": "71bffbef-7302-4cf0-c02f-18c3fe30bb73"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su8OmUfQprNu",
        "outputId": "7f503e98-26ef-40eb-c77f-f0fa5ecf5597"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- day_of_week: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- emp.var.rate: double (nullable = true)\n",
            " |-- cons.price.idx: double (nullable = true)\n",
            " |-- cons.conf.idx: double (nullable = true)\n",
            " |-- euribor3m: double (nullable = true)\n",
            " |-- nr.employed: double (nullable = true)\n",
            " |-- target_class: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1iPUyEKrYRe",
        "outputId": "f1e271fc-3d81-4364-be96-13ae33831356"
      },
      "source": [
        "df.groupBy('target_class').count().show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "|target_class|count|\n",
            "+------------+-----+\n",
            "|          no|36548|\n",
            "|         yes| 4640|\n",
            "+------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqLNWnZ9rGFr"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import *\n",
        "df = df.withColumn('label', F.when(df.target_class == 'no',F.lit(0)).otherwise(F.lit(1)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrY7OqdjGBdr",
        "outputId": "2532fc73-e32d-489c-d305-d78cc9d85422"
      },
      "source": [
        "df.groupBy('label').count().show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1| 4640|\n",
            "|    0|36548|\n",
            "+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6n04AZOGG2U",
        "outputId": "f0cbcb20-7875-403e-96c7-041cd036b6bc"
      },
      "source": [
        "df.show(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+------------+-----+\n",
            "|age|      job|marital|  education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|target_class|label|\n",
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+------------+-----+\n",
            "| 56|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0|          no|    0|\n",
            "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0|          no|    0|\n",
            "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0|          no|    0|\n",
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LafKn08THUZR",
        "outputId": "744b4684-544e-4f34-f761-3f38f04ed5fb"
      },
      "source": [
        "columnList = [item[0] for item in df.dtypes if item[1].startswith('string')]\n",
        "print(columnList)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'target_class']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_mhapxYGnOY",
        "outputId": "a9d4d69d-a308-4113-f538-805da1e9bbeb"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "for col in columnList:\n",
        "  stringIndexer = StringIndexer(inputCol=col,outputCol = col+\"_index\")\n",
        "  indexer = stringIndexer.fit(df).transform(df)\n",
        "  OHE = OneHotEncoder(inputCol=col+\"_index\",outputCol=col+\"_OHE\")\n",
        "  df = OHE.fit(indexer).transform(indexer)\n",
        "df.show(3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+------------+-----+---------+--------------+-------------+-------------+---------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-----------+-----------+-------------+-----------------+---------------+--------------+-------------+------------------+----------------+\n",
            "|age|      job|marital|  education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|target_class|label|job_index|       job_OHE|marital_index|  marital_OHE|education_index|education_OHE|default_index|  default_OHE|housing_index|  housing_OHE|loan_index|     loan_OHE|contact_index|contact_OHE|month_index|    month_OHE|day_of_week_index|day_of_week_OHE|poutcome_index| poutcome_OHE|target_class_index|target_class_OHE|\n",
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+------------+-----+---------+--------------+-------------+-------------+---------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-----------+-----------+-------------+-----------------+---------------+--------------+-------------+------------------+----------------+\n",
            "| 56|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0|          no|    0|      8.0|(11,[8],[1.0])|          0.0|(3,[0],[1.0])|            4.0|(7,[4],[1.0])|          0.0|(2,[0],[1.0])|          1.0|(2,[1],[1.0])|       0.0|(2,[0],[1.0])|          1.0|  (1,[],[])|        0.0|(9,[0],[1.0])|              1.0|  (4,[1],[1.0])|           0.0|(2,[0],[1.0])|               0.0|   (1,[0],[1.0])|\n",
            "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0|          no|    0|      3.0|(11,[3],[1.0])|          0.0|(3,[0],[1.0])|            1.0|(7,[1],[1.0])|          1.0|(2,[1],[1.0])|          1.0|(2,[1],[1.0])|       0.0|(2,[0],[1.0])|          1.0|  (1,[],[])|        0.0|(9,[0],[1.0])|              1.0|  (4,[1],[1.0])|           0.0|(2,[0],[1.0])|               0.0|   (1,[0],[1.0])|\n",
            "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0|          no|    0|      3.0|(11,[3],[1.0])|          0.0|(3,[0],[1.0])|            1.0|(7,[1],[1.0])|          0.0|(2,[0],[1.0])|          0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          1.0|  (1,[],[])|        0.0|(9,[0],[1.0])|              1.0|  (4,[1],[1.0])|           0.0|(2,[0],[1.0])|               0.0|   (1,[0],[1.0])|\n",
            "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+------------+-----+---------+--------------+-------------+-------------+---------------+-------------+-------------+-------------+-------------+-------------+----------+-------------+-------------+-----------+-----------+-------------+-----------------+---------------+--------------+-------------+------------------+----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnuRv317IX9E"
      },
      "source": [
        "vec_assembler = VectorAssembler(inputCols=['age','job_OHE','marital_OHE','education_OHE','default_OHE','housing_OHE','loan_OHE','contact_OHE','month_OHE','day_of_week_OHE','poutcome_OHE'],\n",
        "                                outputCol='features').transform(df)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLbGnz80JtCj"
      },
      "source": [
        "model_df = vec_assembler.select(['features','label'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkwor_mOKT7A"
      },
      "source": [
        "train,test = model_df.randomSplit([0.80,0.20])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUZifXc-KlpC",
        "outputId": "d8b90fd8-ab29-42fd-8219-22c939b0f2f6"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr_model = lr.fit(train)\n",
        "print( lr_model.coefficients)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.005722582008627255,0.15727819558514727,-0.14201202760878004,0.01714461405939384,-0.10329218403593356,-0.01428419125299655,0.6440537276177551,0.029192018830323792,0.02627766258907318,0.027447525918169197,0.13981178229791694,0.8259729708954047,-0.17824183779906333,-0.04374697540005107,-0.32496797080210205,0.019570289253074552,-0.07239630574646001,-0.20780005139753924,-0.08310530034373678,-0.07111826315281648,0.03281280902860767,0.12504756827541225,2.0778453542266937,1.5047821345747283,-0.0075844790831816515,0.027171650324812613,0.03920847522135311,-0.0063674354161558635,0.9324290213426493,-1.5745184403207328,-1.5169288509269674,-1.6209402068154237,-0.8619351201882424,-1.6875874872891972,-0.8148595133259126,-0.08090259944744509,-0.23482716808861206,0.3258386614482381,0.045133896689987164,-0.15764459557703878,0.14558419290776223,0.05030179223684514,-2.2701144529281265,-2.2210323072315776]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWNkFW3UK2zv"
      },
      "source": [
        "\n",
        "[In]: lr_summary=lr_model.summary\n",
        "[In]: lr_summary.accuracy\n",
        "[Out]: 0.673079623648364\n",
        "[In]: lr_summary.areaUnderROC\n",
        "[Out]: 0.7186044694483512\n",
        "[In]: lr_summary.weightedRecall\n",
        "[Out]: 0.673079623648364\n",
        "[In]: lr_summary.weightedPrecision\n",
        "[Out]: 0.6750967624018298\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpJIO7DWLLjz",
        "outputId": "0df82efb-f788-40ae-a326-36467fa3c59c"
      },
      "source": [
        "lr_model.summary.precisionByThreshold.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+------------------+\n",
            "|         threshold|         precision|\n",
            "+------------------+------------------+\n",
            "|0.9118768770791803|0.8620689655172413|\n",
            "|0.9024399707564577|0.8620689655172413|\n",
            "|0.8844543665985334|0.8409090909090909|\n",
            "|0.8617592335124054|0.8050847457627118|\n",
            "|0.8493386986015156|0.7891156462585034|\n",
            "|0.8376719742351242|0.7542857142857143|\n",
            "|0.8302654023270768|0.7378640776699029|\n",
            "|0.8220723823193792|0.7457627118644068|\n",
            "|0.8104220305544716|0.7386363636363636|\n",
            "|0.7921248221087368|0.7406143344709898|\n",
            "|0.7743560032993397|0.7445482866043613|\n",
            "|0.7426850179346008|0.7378917378917379|\n",
            "|0.7285806016424878|0.7368421052631579|\n",
            "|0.7183914042091777|0.7439024390243902|\n",
            "|0.7078835780165118|0.7431818181818182|\n",
            "|0.6966839411308392| 0.744136460554371|\n",
            "|0.6855726427077629|0.7354709418837675|\n",
            "| 0.675817374012148|0.7381404174573055|\n",
            "|0.6631235312824618| 0.737410071942446|\n",
            "|0.6472661074834476|0.7316239316239316|\n",
            "+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBgBFJwaLL82",
        "outputId": "72b3e8b6-1761-4043-e182-77f02cf2ae3c"
      },
      "source": [
        "model_predictions = lr_model.transform(test)\n",
        "model_predictions.columns\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['features', 'label', 'rawPrediction', 'probability', 'prediction']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q4zIFrbLcz4",
        "outputId": "1880868e-f1be-4ca8-e640-16d4b9fbd0e9"
      },
      "source": [
        "model_predictions.select(['label','probability','prediction']).show(10,False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------------------------------------+----------+\n",
            "|label|probability                             |prediction|\n",
            "+-----+----------------------------------------+----------+\n",
            "|0    |[0.8846318498897413,0.1153681501102587] |0.0       |\n",
            "|0    |[0.8846318498897413,0.1153681501102587] |0.0       |\n",
            "|0    |[0.5018360593101189,0.4981639406898811] |0.0       |\n",
            "|0    |[0.9093786422228192,0.09062135777718083]|0.0       |\n",
            "|0    |[0.9093786422228192,0.09062135777718083]|0.0       |\n",
            "|0    |[0.9079538848993545,0.09204611510064553]|0.0       |\n",
            "|0    |[0.9074745101446231,0.0925254898553769] |0.0       |\n",
            "|0    |[0.9065090303158331,0.09349096968416692]|0.0       |\n",
            "|0    |[0.9030583630397901,0.09694163696020985]|0.0       |\n",
            "|0    |[0.8963475266290245,0.1036524733709755] |0.0       |\n",
            "+-----+----------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6BgyS2GLht2",
        "outputId": "1f8acc80-a601-4517-8410-569bd6f8ce99"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "lr_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "lr_auroc = lr_evaluator.evaluate(model_predictions)\n",
        "print(f'The auroc value of Logistic Regression Model is {lr_auroc}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The auroc value of Logistic Regression Model is 0.7664365083565627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzRjN-2qL4oO",
        "outputId": "67c7cba9-389c-416a-b600-707773c7a3e1"
      },
      "source": [
        "lr_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "lr_aupr = lr_evaluator.evaluate(model_predictions)\n",
        "print(f'The aupr value of Logistic Regression Model is {lr_aupr}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The aupr value of Logistic Regression Model is 0.40137308136205163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRrFlzscL_eT"
      },
      "source": [
        "In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "Recall\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.6701030927835051\n",
        "Precision\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.6478405315614618"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODGgxIXWMOWl"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Y3m3oPMOa8"
      },
      "source": [
        "dt = DecisionTreeClassifier()\n",
        "dt_model = dt.fit(train)\n",
        "model_predictions = dt_model.transform(test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjC7LxLJMOdF",
        "outputId": "f6e71616-6583-4a75-e6c7-93dd9a9502a6"
      },
      "source": [
        "model_predictions.select(['label','probability','prediction']).show(10,False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------------------------------------+----------+\n",
            "|label|probability                             |prediction|\n",
            "+-----+----------------------------------------+----------+\n",
            "|0    |[0.8737654320987654,0.12623456790123458]|0.0       |\n",
            "|0    |[0.8737654320987654,0.12623456790123458]|0.0       |\n",
            "|0    |[0.5449101796407185,0.4550898203592814] |0.0       |\n",
            "|0    |[0.8892603172673677,0.11073968273263235]|0.0       |\n",
            "|0    |[0.8892603172673677,0.11073968273263235]|0.0       |\n",
            "|0    |[0.8892603172673677,0.11073968273263235]|0.0       |\n",
            "|0    |[0.8892603172673677,0.11073968273263235]|0.0       |\n",
            "|0    |[0.8892603172673677,0.11073968273263235]|0.0       |\n",
            "|0    |[0.8892603172673677,0.11073968273263235]|0.0       |\n",
            "|0    |[0.8892603172673677,0.11073968273263235]|0.0       |\n",
            "+-----+----------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7FAPBA9MOfX"
      },
      "source": [
        "\n",
        "[In]: dt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: dt_auroc = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of Decision Tree Classifier Model is {dt_auroc}')\n",
        "[Out]: The auc value of Decision Tree Classifier Model is 0.516199386190993\n",
        "[In]: dt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: dt_aupr = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of Decision Tree Model is {dt_aupr}')\n",
        "[Out]: The aupr value of Decision Tree Model is 0.46771834172588167\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.6907216494845361\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.6661143330571665\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RTJ2ymjMOi2"
      },
      "source": [
        "Step 1: Build and Train SVM Model\n",
        "[In]: from pyspark.ml.classification import LinearSVC\n",
        "[In]: lsvc = LinearSVC()\n",
        "[In]: lsvc_model = lsvc.fit(train)\n",
        "[In]: model_predictions = lsvc_model.transform(test)\n",
        "[In]: model_predictions.columns\n",
        "[Out]: ['features', 'label', 'rawPrediction', 'prediction']\n",
        "[In]:model_predictions.select(['label','prediction']).show(10,False)\n",
        "[Out]:\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: svc_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: svc_auroc = svc_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of SupportVectorClassifier  is {svc_auroc}')\n",
        "[Out]: The auc value of SupportVectorClassifier  is 0.7043772749366973\n",
        "[In]: svc_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: svc_aupr =svc_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of SupportVectorClassifier Model is {svc_aupr}')\n",
        "[Out]: The aupr value of SupportVectorClassifier Model is 0.6567277377856992\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.7774914089347079\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.600132625994695"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQi0Isr6M2w7"
      },
      "source": [
        "Step 1: Build and Train SVM Model\n",
        "[In]: from pyspark.ml.classification import NaiveBayes\n",
        "[In]: nb = NaiveBayes()\n",
        "[In]: nb_model = nb.fit(train)\n",
        "[In]: model_predictions = nb_model.transform(test)\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "1h 7m remaining\n",
        "© Pramod Singh 2019\n",
        "P. SinghLearn PySparkhttps://doi.org/10.1007/978-1-4842-4961-1_6\n",
        "6. Supervised Machine Learning\n",
        "Pramod Singh1 \n",
        "(1)\n",
        "Bangalore, Karnataka, India\n",
        " \n",
        "\n",
        "Machine learning can be broadly divided into four categories: supervised machine learning and unsupervised machine learning and, to a lesser extent, semi-supervised machine learning and reinforcement machine learning. Because supervised machine learning drives a lot of business applications and significantly affects our day-to-day lives, it is considered one of the most important categories.\n",
        "\n",
        "This chapter reviews supervised machine learning, using multiple algorithms. In Chapter 7, we’ll look at unsupervised machine learning. I’ll begin by providing an overview of the different categories of supervised machine learning. In the second section, I will cover various regression methods, and we will build machine learning models, using PySpark’s MLlib library. The third and final section of this chapter focuses on classification, using multiple machine learning algorithms.\n",
        "Supervised Machine Learning Primer\n",
        "In supervised machine learning, as the name suggests, the learning process is supervised, as the machine learning algorithm being used corrects its predictions, based on the actual output. In supervised machine learning, the correct labels or output is already known during the model training phase, and, hence, the error can be reduced accordingly. In short, we try to map the relationship between the input data and output label in such a way as to pick up the signals from the training data and generalize about the unseen data as well. The training of the model consists of comparing the actual output with the predicted output and then making the changes in predictions, to reduce the total error between what is actual and what is predicted. The supervised machine learning process followed is as shown in Figure 6-1.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig1_HTML.jpg\n",
        "Figure 6-1\n",
        "\n",
        "Supervised learning approach\n",
        "The data used for training the model is preprocessed, and features are created accordingly. Once the machine learning model is trained, it can be used to make predictions on the unseen data. So, in the preceding figure, we can see how the model is trained, using input data and how now, the trained model is used to predict whether the new transaction is genuine. This type of learning is predominantly used in cases in which historical data is available and predictions must be made on future data. The further categorization of supervised learning is based on types of output or target variables being used for prediction:\n",
        "\n",
        "    Regression\n",
        "\n",
        "    Classification\n",
        "\n",
        "Regression is used when the target value that is being predicted is continuous or numerical in nature. For example, predicting salary based on a given number of years of experience or education falls under the category of regression.\n",
        "Note\n",
        "\n",
        "Although there are multiple types of regression, in this chapter, I’ll focus on linear regression and some of its associated algorithms, as you’ll see shortly.\n",
        "Classification is used if the target variable is a discrete value or categorical in nature. For example, predicting whether a customer will churn out is a type of classification problem, as shown in Figure 6-2.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig2_HTML.jpg\n",
        "Figure 6-2\n",
        "\n",
        "Types of suprvised tasks\n",
        "Classification tasks can further be broken down into two categories: binary class and multi-class, as shown in Figure 6-3.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig3_HTML.png\n",
        "Figure 6-3\n",
        "\n",
        "Types of classes\n",
        "Binary Classification\n",
        "When the target or output variable contains only up to two categories, it is referred to as binary classification. So, every record in the data can only fall under one of the two groups. For example:\n",
        "\n",
        "    Yes or no\n",
        "\n",
        "    Group A or group B\n",
        "\n",
        "    Sell or not sell\n",
        "\n",
        "    Positive or negative\n",
        "\n",
        "    Accepted or rejected\n",
        "\n",
        "Multi-class Classification\n",
        "When the target or output variable contains more than two categories, it is referred to as multi-class classification. So, there can be multiple groups within the data, and every record can belong to any of the groups. For example:\n",
        "\n",
        "    Yes or no or maybe\n",
        "\n",
        "    Group A or group B or group C\n",
        "\n",
        "    Category 1 or category 2 or category 3 or others\n",
        "\n",
        "    Rank 1 or rank 2 or rank 3 or rank 4 or rank 5\n",
        "\n",
        "Another useful property of supervised learning is that the model’s performance can be evaluated on training and test data. Based on the type of model (classification or regression), the evaluation metric can be applied, and performance results can be measured. In this chapter, I will cover how to build machine learning models to execute regression and binary classification.\n",
        "Building a Linear Regression Model\n",
        "Linear regression refers to modeling the relationship between a set of independent variables and the output or dependent (numerical) variables. If the input variables include more than one variable, this is known as multivariable linear regression. In short, it is assumed that the dependent variable is a linear combination of other independent variables.\n",
        "$$ \\overline{y}={B}_0+{B}_1\\ast {X}_1+{B}_2\\ast {X}_{2+\\dots } $$\n",
        "Here X1, X2, … are the independent variables that are used to predict the output variable. The output of the linear regression is a straight line, which minimizes the actual vs. predicted values. A linear regression model cannot handle nonlinear data, as it’s only possible to model linearly separable data, therefore polynomial regression is used for nonlinear data, and the output is generally a curve, instead of a straight line, as shown in Figure 6-4.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig4_HTML.jpg\n",
        "Figure 6-4\n",
        "\n",
        "Types of regression\n",
        "\n",
        "Linear regression also assumes that data is normally distributed, in order to improve prediction. Linear regression is one of the ways to predict continuous values, and you will see now how we can use other alternatives, to predict numerical output.\n",
        "\n",
        "The following sections focus on solving regression tasks, using multiple machine learning algorithms. I will begin with data ingestion and exploratory data analysis and then build models. The steps 1 to 4 will be the same for all the regression models.\n",
        "Note\n",
        "\n",
        "Complete datasets, along with the relevant code, are available for reference from the GitHub repository for this book and execute best on Spark 2.3 and higher versions.\n",
        "\n",
        "Let’s build a linear regression model, using Spark’s MLlib library, and predict the target variable, using the input features.\n",
        "Reviewing the Data Information\n",
        "\n",
        "The dataset that we are going to use for this example is a sample dataset and contains a total of 1,232 rows and 6 columns. We have to use 5 input variables to predict the target variable, using the linear regression model.\n",
        "Step 1: Create the Spark Session Object\n",
        "We start the Jupyter notebook, import SparkSession, and create a new SparkSession object to use with Spark.\n",
        "[In]: from pyspark.sql import SparkSession\n",
        "[In]: spark=SparkSession.builder.appName('supervised_ml').getOrCreate()\n",
        "Step 2: Read the Dataset\n",
        "We then load and read the dataset within Spark, using DataFrame. We have to make sure that we have opened PySpark from the same directory folder where the dataset is available, or else we have to mention the directory path of the data folder.\n",
        "[In]: df=spark.read.csv('Linear_regression_dataset.csv',inferSchema=True,header=True)\n",
        "[In]:print((df.count(), len(df.columns)))\n",
        "[Out]: (1232, 6)\n",
        "The preceding output confirms the size of our dataset, so we can then validate the datatypes of the input values, to check if we have to change/cast any column datatypes. In this example, all columns contain integer or double values that are already aligned with our requirements.\n",
        "[In]: df.printSchema()\n",
        "[Out]: root\n",
        " |-- var_1: integer (nullable = true)\n",
        " |-- var_2: integer (nullable = true)\n",
        " |-- var_3: integer (nullable = true)\n",
        " |-- var_4: double (nullable = true)\n",
        " |-- var_5: double (nullable = true)\n",
        " |-- output: double (nullable = true)\n",
        "There is a total of six columns, of which five are input columns (var_1 to var_5) and target columns (label). We can now use the describe function to go over statistical measures of the dataset.\n",
        "[In]: df.show(10)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figa_HTML.jpg\n",
        "Step 3: Feature Engineering\n",
        "This is the part where we create a single vector combining all input features, by using Spark’s VectorAssembler. It creates only a single feature that captures the input values for that particular row. So, instead of five input columns, the engine essentially translates the features into a single column with five input values, in the form of a list.\n",
        "[In]: from pyspark.ml.linalg import Vector\n",
        "[In]: from pyspark.ml.feature import VectorAssembler\n",
        "We will pass all five input columns, to create a single features column.\n",
        "[In]: df.columns\n",
        "[Out]: ['var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'label']\n",
        "[In]: vec_assmebler=VectorAssembler(inputCols=['var_1', 'var_2', 'var_3', 'var_4', 'var_5'],outputCol='features')\n",
        "[In]: features_df=vec_assmebler.transform(df)\n",
        "[In]: features_df.printSchema()\n",
        "[Out]: root\n",
        " |-- var_1: integer (nullable = true)\n",
        " |-- var_2: integer (nullable = true)\n",
        " |-- var_3: integer (nullable = true)\n",
        " |-- var_4: double (nullable = true)\n",
        " |-- var_5: double (nullable = true)\n",
        " |-- label: double (nullable = true)\n",
        " |-- features: vector (nullable = true)\n",
        "As you can see, we have an additional column (features), which contains the single dense vector for all of the inputs. We then take a subset of the dataframe and select only the features column and the label column, to build the linear regression model.\n",
        "[In]: df.select(['features','label']).show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figb_HTML.jpg\n",
        "Step 4: Split the Dataset\n",
        "Let’s split the dataset into training and test datasets, in order to train and evaluate the performance of the linear regression model. We split it according to a 70/30 ratio and train our model on 70% of the dataset. We can print the shape of the train and test data, to validate the size.\n",
        "[In]: train, test = df.randomSplit([0.75, 0.25])\n",
        "[In]:print(f\"Size of train Dataset : {train.count()}\" )\n",
        "[Out]: Size of train Dataset : 911\n",
        "[In]: print(f\"Size of test Dataset : {test.count()}\" )\n",
        "[Out]:  Size of test Dataset : 321\n",
        "Step 5: Build and Train Linear Regression Model\n",
        "Now we build and train the linear regression model, using features, input, and label columns. We first import the linear regression from MLlib, as follows:\n",
        "[In]: from pyspark.ml.regression import LinearRegression\n",
        "[In]: lr = LinearRegression()\n",
        "Note\n",
        "\n",
        "For simplicity, all the machine learning models built in this chapter use default hyperparameters. Readers can use their own set of hyperparameters.\n",
        "[In]:lr_model = lr.fit(train)\n",
        "[In]: predictions_df=lr_model.transform(test)\n",
        "[In]: predictions_df.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figc_HTML.jpg\n",
        "Step 6: Evaluate Linear Regression Model on Test Data\n",
        "To check the performance of the model on unseen or test data, we make use of evaluate.\n",
        "[In]: model_predictions=lr_model.evaluate(test)\n",
        "[In]: model_predictions.r2\n",
        "[Out]: 0.8855561089304634\n",
        "[In]: print(model_predictions.meanSquaredError)\n",
        "[Out]:0.00013305453514672318\n",
        "Generalized Linear Model Regression\n",
        "The generalized linear model (GLM) is an advanced version of linear regression that considers the target variable to have an error distribution other than a preferred normal distribution. The GLM generalizes linear regression, using a link function, so that variance is a function of the predicted value itself. Let’s try to build the GLM on the same dataset and see if it performs better than a simple linear regression model. First, we must import the GLM from MLlib.\n",
        "[In]: from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "Step 1: Build and Train Generalized Linear Regression Model\n",
        "[In]: glr = GeneralizedLinearRegression()\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: glr_model.coefficients\n",
        "[Out]: DenseVector([0.0003, 0.0001, 0.0001, -0.6374, 0.4822])\n",
        "We can get the coefficient values, using coefficient functions of that model. Here we can see that one of the features has a negative coefficient value. We can get more information about the GLM model, by using the summary function. It returns all the details, such as coefficient value, std error, AIC (Akaike information criterion) value, and p value.\n",
        "[In]: glr_model.summary\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figd_HTML.jpg\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "[In]: model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fige_HTML.jpg\n",
        "The Akaike information criterion (AIC) is an evaluation parameter of relative performance of quality of models for the same set dataset. AIC is mainly used to select among multiple models for a given dataset. A lesser value of AIC indicates that the model is of good quality. AIC tries to strike a balance between the variance and bias of the model. Therefore, it deals with the chances both of overfitting and underfitting. The model with the lowest AIC score is preferred over other models.\n",
        "[In]: model_predictions.aic\n",
        "[Out]: -1939.88\n",
        "We can run the GLM for multiple distributions, such as\n",
        "\n",
        "    1.\n",
        "\n",
        "    Binomial\n",
        "     \n",
        "    2.\n",
        "\n",
        "    Poisson\n",
        "     \n",
        "    3.\n",
        "\n",
        "    Gamma\n",
        "     \n",
        "    4.\n",
        "\n",
        "    Tweedie\n",
        "     \n",
        "\n",
        "[In]: glr = GeneralizedLinearRegression(family='Binomial')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.aic\n",
        "[Out]: 336.991\n",
        "[In]: glr = GeneralizedLinearRegression(family='Poisson')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: predictions=glr_model.evaluate(test)\n",
        "[In]: predictions.aic\n",
        "[Out]: 266.53\n",
        "[In]: glr = GeneralizedLinearRegression(family='Gamma')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]:model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.aic\n",
        "[Out]: -1903.81\n",
        "\n",
        "Here we can see that our default GLM model with Gaussian distribution has the lowest AIC value, compared to others.\n",
        "Decision Tree Regression\n",
        "\n",
        "The decision tree regression algorithm can be used for both regression and classification. It is quite powerful in terms of fitting the data well but comes with the high risk of sometimes overfitting the data. Decision trees contain multiple splits based on entropy or Gini indexes. The deeper the tree, the higher the chance of overfitting the data. In our example, we will build a decision tree for predicting the target value, with the default value of parameters (maxdepth = 5).\n",
        "Step 1: Build and Train Decision Tree Regressor Model\n",
        "[In]: from pyspark.ml.regression import DecisionTreeRegressor\n",
        "[In]: dec_tree = DecisionTreeRegressor()\n",
        "[In]: dec_tree_model = dec_tree.fit(train)\n",
        "[In]: dec_tree_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.9641, 1: 0.0193, 2: 0.0029, 3: 0.0053, 4: 0.0084})\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "[In]: model_predictions = dec_tree_model.transform(test)\n",
        "[In]: model_predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figf_HTML.jpg\n",
        "We import RegressionEvaluation from MLlib, to evaluate the performance of the decision tree on test data. As of now, there are two metrics available for evaluation: r2 and RMSE (root mean squared error). r2 mainly suggests how much of the variation in the dataset can be attributed to regression. Therefore, the higher the r2, the better the performance of the model. On the other hand, RMSE suggests the total errors the model is making, in terms of the difference between actual and predicted values.\n",
        "[In]: from pyspark.ml.evaluation import RegressionEvaluator\n",
        "[In]: dt_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: dt_r2 = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of DecisionTreeRegressor is {dt_r2}')\n",
        "[Out]: The r-square value of DecisionTreeRegressor is 0.8093834699203476\n",
        "[In]: dt_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: dt_rmse = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of DecisionTreeRegressor is {dt_rmse}')\n",
        "[Out]: The rmse value of DecisionTreeRegressor is 0.014111932287681688\n",
        "\n",
        "The r2 value of this particular model is close to 0.81, which is a little lower than that of a simple linear regression model.\n",
        "Random Forest Regressors\n",
        "\n",
        "Random forest regressors are a collection of multiple individual decision trees built using different samples of data. The whole idea of combining these individual trees is to take majority voting or averages (in case of regression) to generalize effectively. A random forest is, therefore, an ensembling technique that takes a bagging approach. It can be used for regression as well as for classification tasks. Because decision trees tend to overfit the data, random forests remove the element of high variance, by taking the means of the predicted values from individual trees. In our example, we will build a random forest model for regression, using default parameters (numTrees = 20)\n",
        "Step 1: Build and Train Random Forest Regressor Model\n",
        "[In]: from pyspark.ml.regression import RandomForestRegressor\n",
        "[In]: rf = RandomForestRegressor()\n",
        "[In]: rf_model = rf.fit(train)\n",
        "[In]:  rf_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.4395, 1: 0.045, 2: 0.0243, 3: 0.2725, 4: 0.2188})\n",
        "As you can see, the number of trees in the random forest is equal to 20. This number can be increased.\n",
        "[In]: rf_model.getNumTrees\n",
        "[Out]: 20\n",
        "[In]: model_predictions = rf_model.transform(test)\n",
        "[In]: model_predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figg_HTML.jpg\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "We can again use r2and RMSE as the evaluation parameter of the random forest model.\n",
        "[In]:rf_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: rf_r2 = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of RandomForestRegressor is {rf_r2}')\n",
        "[Out]: The r-square value of RandomForestRegressor is 0.8215863293044671\n",
        "[In]: rf_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: rf_rmse = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of RandomForestRegressor is {rf_rmse}')\n",
        "[Out]: The rmse value of RandomForestRegressor is 0.01365275410722947\n",
        "\n",
        "As you can see, it clearly outperforms the decision tree regressor and has a higher r2. The performance of this model can further be enhanced with hyperparameter tuning.\n",
        "Gradient-Boosted Tree Regressor\n",
        "\n",
        "A gradient-boosted tree (GBT) regressor is also an ensembling technique, which uses boosting under the hood. Boosting refers to making use of individual weak learners in order to boost the performance of the overall model. One major difference between bagging and boosting is that in bagging, the individual models that are built are parallel in nature, meaning they can be built independent of each other, but in boosting, the individual models are built in a sequential manner. In a gradient boosting approach, the second model focuses on the errors made by the first model and tries to reduce overall errors for those data points. Similarly, the next model tries to reduce the errors made by the previous model. In this way, the overall error of prediction is reduced. In the following example, we will build a GBT regressor with default parameters.\n",
        "Step 1: Build and Train a GBT Regressor Model\n",
        "[In]: from pyspark.ml.regression import GBTRegressor\n",
        "[In]: gbt = GBTRegressor()\n",
        "[In]: gbt_model=gbt.fit(train)\n",
        "[In]: gbt_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.2325, 1: 0.2011, 2: 0.1645, 3: 0.2268, 4: 0.1751})\n",
        "[In]: model_predictions = gbt_model.transform(test)\n",
        "[In]: model_predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figh_HTML.jpg\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "[In]: gbt_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: gbt_r2 = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of GradientBoostedRegressor is {gbt_r2}')\n",
        "[Out]: The r-square value of GradientBoostedRegressor is 0.8477273892307596\n",
        "[In]: gbt_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: gbt_rmse = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of GradientBoostedRegressor is {gbt_rmse}')\n",
        "[Out]: The rmse value of GradientBoostedRegressor is 0.013305445803592103\n",
        "\n",
        "As you can see, the GBT regressor outperforms the random forestmodel. With r2 being close to 0.85, it can be considered the final model, after proper tuning.\n",
        "Building Multiple Models for Binary Classification Tasks\n",
        "\n",
        "In this third and final section of the chapter, you will see how to build multiple machine learning models for binary classification tasks. The data that we are going to use for this is a subset of an open source Bank Marketing Data Set from the UCI ML repository, available at https://archive.ics.uci.edu/ml/datasets/Bank+Marketing.\n",
        "\n",
        "There are two reasons for selecting only a subset of this data. The first is to maintain the class balance for the classification task, so as not to make it an anomalous detection category task. Another reason for selecting only a subset of the features is to limit the amount of signals in the data, as some of the features in the dataset strongly affect the output and, therefore, are ignored in this exercise.\n",
        "\n",
        "The dataset contains 9,500 rows and 8 columns. The idea is to predict if the user will subscribe to another product or service (term deposit), based on the other attributes, such as age, job, loan, etc. This is a typical requirement in which machine learning is leveraged to find the top users who can be targeted by the business for cross-selling or upselling.\n",
        "\n",
        "I’ll begin with the logistic regression model.\n",
        "Logistic Regression\n",
        "\n",
        "Logistic regression is considered to be one of baseline models, owing to its simplicity and interpretability. Under the hood, it is quite similar to linear regression. It also assumes that output is a linear combination of the dependent variables, but to keep the output between 0 and 1, as it returns the probability as output, it makes use of a nonlinear function (sigmoid), which produces an S curve instead of a straight line (linear regression).\n",
        "\n",
        "We’ll start by building the baseline in Steps 1–3 and then complete the logistic regression model with default hyperparameters, in Steps 4–6.\n",
        "Step 1: Read the Dataset\n",
        "[In]: df=spark.read.csv('bank_data.csv',inferSchema=True,header=True)\n",
        "[In]: df.count()\n",
        "[Out]: 9501\n",
        "[In]: df.columns\n",
        "[Out]: ['age', 'job', 'marital', 'education', 'default', 'housing','loan', 'target_class']\n",
        "[In]: df.printSchema()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figi_HTML.jpg\n",
        "As you can see, the input columns are all the columns, except for the target class column. The target class is also well-balanced, in terms of the count of yes and no labels. We will have to convert yeses and noes into 1s and 0s, as well as rename the target_class column to “label,” which is the default acceptance column name in machine learning model parameters.\n",
        "[In]: df.groupBy('target_class').count().show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figj_HTML.jpg\n",
        "Step 2: Feature Engineering for Model\n",
        "[In]: from pyspark.sql import functions as F\n",
        "[In]: from pyspark.sql import ∗\n",
        "[In]: df=df.withColumn(\"label\", F.when(df.target_class =='no', F.lit(0)).otherwise(F.lit(1)))\n",
        "[In]: df.groupBy('label').count().show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figk_HTML.jpg\n",
        "Now that we have renamed the output column “label” and converted the target class to 1s and 0s, the next step is to create features for the model. Because we have categorical columns, such as job and edu, we will have to use StringIndexer and OneHotEncoder to convert them into a numerical format. We create a Python function, cat_to_num, to convert all the categorical features into numerical ones.\n",
        "[In]: from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "[In]: def cat_to_num(df):\n",
        "    for col in df.columns:\n",
        "        stringIndexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n",
        "        model = stringIndexer.fit(df)\n",
        "        indexed = model.transform(df)\n",
        "        encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\")\n",
        "        df = encoder.transform(indexed)\n",
        "    df_assembler = VectorAssembler(inputCols=['age','marital_vec','education_vec','default_vec','housing_vec','loan_vec'], outputCol=\"features\")\n",
        "    df = df_assembler.transform(df)\n",
        "    return df.select(['features','label'])\n",
        "We just select the new features column and target label column, as we don’t need the earlier original columns for model training.\n",
        "[In]: df_new=cat_to_num(df)\n",
        "[In]: df_new.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figl_HTML.jpg\n",
        "\n",
        "Now we have all the input features merged into a single dense vector ('features'), along with output column labels, which we can use to train the machine learning models. The new dataframe created using only two columns (features, label) is now called df_new and will be used for every model. We can now split this new dataframe into train and test datasets. We can split the data into a 75%/25% ratio, using the randomplit function.\n",
        "Step 3: Split the Data into Train and Test Datasets\n",
        "[In]: train, test = df_new.randomSplit([0.75, 0.25])\n",
        "[In]: print(f\"Size of train Dataset : {train.count()}\" )\n",
        "[Out]: 7121\n",
        "[In]: print(f\"Size of test Dataset : {test.count()}\" )\n",
        "[Out]: 2380\n",
        "Step 4: Build and Train the Logistic Regression Model\n",
        "[In]: from pyspark.ml.classification import LogisticRegression\n",
        "[In]: lr = LogisticRegression()\n",
        "[In]: lr_model = lr.fit(train)\n",
        "[In]:print( lr_model.coefficients)\n",
        "[Out]:\n",
        "[0.0272019114172,-0.647672064875,0.229030508111,-0.77074788287,-12.36869511,-12.8865599132,-13.2257790609,-12.6705131313,-13.0023164274,-13.0747662586,-12.6985757761,1.42220523957,0.301582233094,-0.0127231892838,0.218471149577,0.332362933568]\n",
        "\n",
        "Once the model is built, we can make use of the internal function summary, which offers important details regarding the model, such as ROC curve, precision, recall, AUC (area under the curve), etc.\n",
        "Step 5: Evaluate Performance on Training Data\n",
        "[In]: lr_summary=lr_model.summary\n",
        "[In]: lr_summary.accuracy\n",
        "[Out]: 0.673079623648364\n",
        "[In]: lr_summary.areaUnderROC\n",
        "[Out]: 0.7186044694483512\n",
        "[In]: lr_summary.weightedRecall\n",
        "[Out]: 0.673079623648364\n",
        "[In]: lr_summary.weightedPrecision\n",
        "[Out]: 0.6750967624018298\n",
        "Here, using the summary function , we can view the model’s performance on train data, such as its accuracy, AUC, weighted recall, and precision. We can also view additional details—such as how precision varies for various threshold values, the relation between precision and recall, and how recall varies with different threshold values—to pick the right threshold value for the model. These also can be plotted, to view the relationships.\n",
        "[In]: lr_summary.precisionByThreshold.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figm_HTML.jpg\n",
        "[In]: lr_summary.roc.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fign_HTML.jpg\n",
        "[In]: lr_summary.recallByThreshold.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figo_HTML.jpg\n",
        "[In]: lr_summary.pr.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figp_HTML.jpg\n",
        "Step 6: Evaluate Performance on Test Data\n",
        "[In]: model_predictions = lr_model.transform(test)\n",
        "[In]: model_predictions.columns\n",
        "[Out]: ['features', 'label', 'rawPrediction', 'probability', 'prediction']\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figq_HTML.jpg\n",
        "As you can see, the prediction column shows the model prediction for each of the records in the test data. The probability column shows the values for both classes (0 & 1). The probability at 0th index is of 0; the other is for a prediction of 1. The evaluation of the logistic regression model on test data can be done using BinaryClassEvaluator . We can get the area under ROC and that under the PR curve, as shown following:\n",
        "[In]:from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "[In]:lr_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: lr_auroc = lr_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auroc value of Logistic Regression Model is {lr_auroc}')\n",
        "[Out]: The auroc value of Logistic Regression Model is 0.7092938229110143\n",
        "[In]: lr_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: lr_aupr = lr_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of Logistic Regression Model is {lr_aupr}')\n",
        "[Out]: The aupr value of Logistic Regression Model is 0.6630743130940658\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "Recall\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.6701030927835051\n",
        "Precision\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.6478405315614618\n",
        "Decision Tree Classifier\n",
        "\n",
        "As mentioned earlier, decision trees can be used for classification as well as regression. Here, we will build a decision tree with default hyperparameters and use it to predict whether the user will opt for the new term deposit plan.\n",
        "Step 1: Build and Train Decision Tree Classifier Model\n",
        "[In]: from pyspark.ml.classification import DecisionTreeClassifier\n",
        "[In]: dt = DecisionTreeClassifier()\n",
        "[In]: dt_model = dt.fit(train)\n",
        "[In]: model_predictions = dt_model.transform(test)\n",
        "[Out]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figr_HTML.jpg\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: dt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: dt_auroc = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of Decision Tree Classifier Model is {dt_auroc}')\n",
        "[Out]: The auc value of Decision Tree Classifier Model is 0.516199386190993\n",
        "[In]: dt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: dt_aupr = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of Decision Tree Model is {dt_aupr}')\n",
        "[Out]: The aupr value of Decision Tree Model is 0.46771834172588167\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.6907216494845361\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.6661143330571665\n",
        "Support Vector Machines Classifiers\n",
        "Support vector machines (SVMs) are used for classification tasks, as they find the hyperplane that maximizes the margin (perpendicular distance) between two classes. All the instances and target classes are represented as vectors in high-dimensional space, and the SVM finds the closest two points from the two classes that support the best separating line or hyperplane, as shown in Figure 6-5.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig5_HTML.jpg\n",
        "Figure 6-5\n",
        "\n",
        "Support vector machine\n",
        "\n",
        "For nonlinearly separable data, there are different kernel tricks to separate the classes. In our example, we will build a linearly separable support vector classifier with default hyperparameters.\n",
        "Step 1: Build and Train SVM Model\n",
        "[In]: from pyspark.ml.classification import LinearSVC\n",
        "[In]: lsvc = LinearSVC()\n",
        "[In]: lsvc_model = lsvc.fit(train)\n",
        "[In]: model_predictions = lsvc_model.transform(test)\n",
        "[In]: model_predictions.columns\n",
        "[Out]: ['features', 'label', 'rawPrediction', 'prediction']\n",
        "[In]:model_predictions.select(['label','prediction']).show(10,False)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figs_HTML.jpg\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: svc_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: svc_auroc = svc_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of SupportVectorClassifier  is {svc_auroc}')\n",
        "[Out]: The auc value of SupportVectorClassifier  is 0.7043772749366973\n",
        "[In]: svc_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: svc_aupr =svc_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of SupportVectorClassifier Model is {svc_aupr}')\n",
        "[Out]: The aupr value of SupportVectorClassifier Model is 0.6567277377856992\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.7774914089347079\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.600132625994695\n",
        "Naive Bayes Classifier\n",
        "\n",
        "Naive Bayes (NB) classifiers work on the principle of conditional probability and assume absolute independence between predictors. An NB classifier doesn’t have many hyperparameters and can outperform some of the most sophisticated algorithms out there. In the following example, we will build an NB classifier and evaluate its performance on the test data.\n",
        "Step 1: Build and Train SVM Model\n",
        "[In]: from pyspark.ml.classification import NaiveBayes\n",
        "[In]: nb = NaiveBayes()\n",
        "[In]: nb_model = nb.fit(train)\n",
        "[In]: model_predictions = nb_model.transform(test)\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "[Out]:\n",
        "\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: nb_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: nb_auroc = nb_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of NB Classifier is {nb_auroc}')\n",
        "[Out]: The auc value of NB Classifier is 0.43543736717760884\n",
        "[In]: nb_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: nb_aupr =nb_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of NB Classifier Model is {nb_aupr}')\n",
        "[Out]: The aupr value of NB Classifier Model is 0.4321001351769349\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.586\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.625\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQEi6u4OM21U"
      },
      "source": [
        "Step 1: Build and Train the GBT Model\n",
        "[In]: from pyspark.ml.classification import GBTClassifier\n",
        "[In]: gbt = GBTClassifier()\n",
        "[In]: gbt_model = gbt.fit(train)\n",
        "[In]: model_predictions = gbt_model.transform(test)\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "\n",
        "1h 5m remaining\n",
        "© Pramod Singh 2019\n",
        "P. SinghLearn PySparkhttps://doi.org/10.1007/978-1-4842-4961-1_6\n",
        "6. Supervised Machine Learning\n",
        "Pramod Singh1 \n",
        "(1)\n",
        "Bangalore, Karnataka, India\n",
        " \n",
        "\n",
        "Machine learning can be broadly divided into four categories: supervised machine learning and unsupervised machine learning and, to a lesser extent, semi-supervised machine learning and reinforcement machine learning. Because supervised machine learning drives a lot of business applications and significantly affects our day-to-day lives, it is considered one of the most important categories.\n",
        "\n",
        "This chapter reviews supervised machine learning, using multiple algorithms. In Chapter 7, we’ll look at unsupervised machine learning. I’ll begin by providing an overview of the different categories of supervised machine learning. In the second section, I will cover various regression methods, and we will build machine learning models, using PySpark’s MLlib library. The third and final section of this chapter focuses on classification, using multiple machine learning algorithms.\n",
        "Supervised Machine Learning Primer\n",
        "In supervised machine learning, as the name suggests, the learning process is supervised, as the machine learning algorithm being used corrects its predictions, based on the actual output. In supervised machine learning, the correct labels or output is already known during the model training phase, and, hence, the error can be reduced accordingly. In short, we try to map the relationship between the input data and output label in such a way as to pick up the signals from the training data and generalize about the unseen data as well. The training of the model consists of comparing the actual output with the predicted output and then making the changes in predictions, to reduce the total error between what is actual and what is predicted. The supervised machine learning process followed is as shown in Figure 6-1.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig1_HTML.jpg\n",
        "Figure 6-1\n",
        "\n",
        "Supervised learning approach\n",
        "The data used for training the model is preprocessed, and features are created accordingly. Once the machine learning model is trained, it can be used to make predictions on the unseen data. So, in the preceding figure, we can see how the model is trained, using input data and how now, the trained model is used to predict whether the new transaction is genuine. This type of learning is predominantly used in cases in which historical data is available and predictions must be made on future data. The further categorization of supervised learning is based on types of output or target variables being used for prediction:\n",
        "\n",
        "    Regression\n",
        "\n",
        "    Classification\n",
        "\n",
        "Regression is used when the target value that is being predicted is continuous or numerical in nature. For example, predicting salary based on a given number of years of experience or education falls under the category of regression.\n",
        "Note\n",
        "\n",
        "Although there are multiple types of regression, in this chapter, I’ll focus on linear regression and some of its associated algorithms, as you’ll see shortly.\n",
        "Classification is used if the target variable is a discrete value or categorical in nature. For example, predicting whether a customer will churn out is a type of classification problem, as shown in Figure 6-2.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig2_HTML.jpg\n",
        "Figure 6-2\n",
        "\n",
        "Types of suprvised tasks\n",
        "Classification tasks can further be broken down into two categories: binary class and multi-class, as shown in Figure 6-3.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig3_HTML.png\n",
        "Figure 6-3\n",
        "\n",
        "Types of classes\n",
        "Binary Classification\n",
        "When the target or output variable contains only up to two categories, it is referred to as binary classification. So, every record in the data can only fall under one of the two groups. For example:\n",
        "\n",
        "    Yes or no\n",
        "\n",
        "    Group A or group B\n",
        "\n",
        "    Sell or not sell\n",
        "\n",
        "    Positive or negative\n",
        "\n",
        "    Accepted or rejected\n",
        "\n",
        "Multi-class Classification\n",
        "When the target or output variable contains more than two categories, it is referred to as multi-class classification. So, there can be multiple groups within the data, and every record can belong to any of the groups. For example:\n",
        "\n",
        "    Yes or no or maybe\n",
        "\n",
        "    Group A or group B or group C\n",
        "\n",
        "    Category 1 or category 2 or category 3 or others\n",
        "\n",
        "    Rank 1 or rank 2 or rank 3 or rank 4 or rank 5\n",
        "\n",
        "Another useful property of supervised learning is that the model’s performance can be evaluated on training and test data. Based on the type of model (classification or regression), the evaluation metric can be applied, and performance results can be measured. In this chapter, I will cover how to build machine learning models to execute regression and binary classification.\n",
        "Building a Linear Regression Model\n",
        "Linear regression refers to modeling the relationship between a set of independent variables and the output or dependent (numerical) variables. If the input variables include more than one variable, this is known as multivariable linear regression. In short, it is assumed that the dependent variable is a linear combination of other independent variables.\n",
        "$$ \\overline{y}={B}_0+{B}_1\\ast {X}_1+{B}_2\\ast {X}_{2+\\dots } $$\n",
        "Here X1, X2, … are the independent variables that are used to predict the output variable. The output of the linear regression is a straight line, which minimizes the actual vs. predicted values. A linear regression model cannot handle nonlinear data, as it’s only possible to model linearly separable data, therefore polynomial regression is used for nonlinear data, and the output is generally a curve, instead of a straight line, as shown in Figure 6-4.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig4_HTML.jpg\n",
        "Figure 6-4\n",
        "\n",
        "Types of regression\n",
        "\n",
        "Linear regression also assumes that data is normally distributed, in order to improve prediction. Linear regression is one of the ways to predict continuous values, and you will see now how we can use other alternatives, to predict numerical output.\n",
        "\n",
        "The following sections focus on solving regression tasks, using multiple machine learning algorithms. I will begin with data ingestion and exploratory data analysis and then build models. The steps 1 to 4 will be the same for all the regression models.\n",
        "Note\n",
        "\n",
        "Complete datasets, along with the relevant code, are available for reference from the GitHub repository for this book and execute best on Spark 2.3 and higher versions.\n",
        "\n",
        "Let’s build a linear regression model, using Spark’s MLlib library, and predict the target variable, using the input features.\n",
        "Reviewing the Data Information\n",
        "\n",
        "The dataset that we are going to use for this example is a sample dataset and contains a total of 1,232 rows and 6 columns. We have to use 5 input variables to predict the target variable, using the linear regression model.\n",
        "Step 1: Create the Spark Session Object\n",
        "We start the Jupyter notebook, import SparkSession, and create a new SparkSession object to use with Spark.\n",
        "[In]: from pyspark.sql import SparkSession\n",
        "[In]: spark=SparkSession.builder.appName('supervised_ml').getOrCreate()\n",
        "Step 2: Read the Dataset\n",
        "We then load and read the dataset within Spark, using DataFrame. We have to make sure that we have opened PySpark from the same directory folder where the dataset is available, or else we have to mention the directory path of the data folder.\n",
        "[In]: df=spark.read.csv('Linear_regression_dataset.csv',inferSchema=True,header=True)\n",
        "[In]:print((df.count(), len(df.columns)))\n",
        "[Out]: (1232, 6)\n",
        "The preceding output confirms the size of our dataset, so we can then validate the datatypes of the input values, to check if we have to change/cast any column datatypes. In this example, all columns contain integer or double values that are already aligned with our requirements.\n",
        "[In]: df.printSchema()\n",
        "[Out]: root\n",
        " |-- var_1: integer (nullable = true)\n",
        " |-- var_2: integer (nullable = true)\n",
        " |-- var_3: integer (nullable = true)\n",
        " |-- var_4: double (nullable = true)\n",
        " |-- var_5: double (nullable = true)\n",
        " |-- output: double (nullable = true)\n",
        "There is a total of six columns, of which five are input columns (var_1 to var_5) and target columns (label). We can now use the describe function to go over statistical measures of the dataset.\n",
        "[In]: df.show(10)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figa_HTML.jpg\n",
        "Step 3: Feature Engineering\n",
        "This is the part where we create a single vector combining all input features, by using Spark’s VectorAssembler. It creates only a single feature that captures the input values for that particular row. So, instead of five input columns, the engine essentially translates the features into a single column with five input values, in the form of a list.\n",
        "[In]: from pyspark.ml.linalg import Vector\n",
        "[In]: from pyspark.ml.feature import VectorAssembler\n",
        "We will pass all five input columns, to create a single features column.\n",
        "[In]: df.columns\n",
        "[Out]: ['var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'label']\n",
        "[In]: vec_assmebler=VectorAssembler(inputCols=['var_1', 'var_2', 'var_3', 'var_4', 'var_5'],outputCol='features')\n",
        "[In]: features_df=vec_assmebler.transform(df)\n",
        "[In]: features_df.printSchema()\n",
        "[Out]: root\n",
        " |-- var_1: integer (nullable = true)\n",
        " |-- var_2: integer (nullable = true)\n",
        " |-- var_3: integer (nullable = true)\n",
        " |-- var_4: double (nullable = true)\n",
        " |-- var_5: double (nullable = true)\n",
        " |-- label: double (nullable = true)\n",
        " |-- features: vector (nullable = true)\n",
        "As you can see, we have an additional column (features), which contains the single dense vector for all of the inputs. We then take a subset of the dataframe and select only the features column and the label column, to build the linear regression model.\n",
        "[In]: df.select(['features','label']).show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figb_HTML.jpg\n",
        "Step 4: Split the Dataset\n",
        "Let’s split the dataset into training and test datasets, in order to train and evaluate the performance of the linear regression model. We split it according to a 70/30 ratio and train our model on 70% of the dataset. We can print the shape of the train and test data, to validate the size.\n",
        "[In]: train, test = df.randomSplit([0.75, 0.25])\n",
        "[In]:print(f\"Size of train Dataset : {train.count()}\" )\n",
        "[Out]: Size of train Dataset : 911\n",
        "[In]: print(f\"Size of test Dataset : {test.count()}\" )\n",
        "[Out]:  Size of test Dataset : 321\n",
        "Step 5: Build and Train Linear Regression Model\n",
        "Now we build and train the linear regression model, using features, input, and label columns. We first import the linear regression from MLlib, as follows:\n",
        "[In]: from pyspark.ml.regression import LinearRegression\n",
        "[In]: lr = LinearRegression()\n",
        "Note\n",
        "\n",
        "For simplicity, all the machine learning models built in this chapter use default hyperparameters. Readers can use their own set of hyperparameters.\n",
        "[In]:lr_model = lr.fit(train)\n",
        "[In]: predictions_df=lr_model.transform(test)\n",
        "[In]: predictions_df.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figc_HTML.jpg\n",
        "Step 6: Evaluate Linear Regression Model on Test Data\n",
        "To check the performance of the model on unseen or test data, we make use of evaluate.\n",
        "[In]: model_predictions=lr_model.evaluate(test)\n",
        "[In]: model_predictions.r2\n",
        "[Out]: 0.8855561089304634\n",
        "[In]: print(model_predictions.meanSquaredError)\n",
        "[Out]:0.00013305453514672318\n",
        "Generalized Linear Model Regression\n",
        "The generalized linear model (GLM) is an advanced version of linear regression that considers the target variable to have an error distribution other than a preferred normal distribution. The GLM generalizes linear regression, using a link function, so that variance is a function of the predicted value itself. Let’s try to build the GLM on the same dataset and see if it performs better than a simple linear regression model. First, we must import the GLM from MLlib.\n",
        "[In]: from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "Step 1: Build and Train Generalized Linear Regression Model\n",
        "[In]: glr = GeneralizedLinearRegression()\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: glr_model.coefficients\n",
        "[Out]: DenseVector([0.0003, 0.0001, 0.0001, -0.6374, 0.4822])\n",
        "We can get the coefficient values, using coefficient functions of that model. Here we can see that one of the features has a negative coefficient value. We can get more information about the GLM model, by using the summary function. It returns all the details, such as coefficient value, std error, AIC (Akaike information criterion) value, and p value.\n",
        "[In]: glr_model.summary\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figd_HTML.jpg\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "[In]: model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fige_HTML.jpg\n",
        "The Akaike information criterion (AIC) is an evaluation parameter of relative performance of quality of models for the same set dataset. AIC is mainly used to select among multiple models for a given dataset. A lesser value of AIC indicates that the model is of good quality. AIC tries to strike a balance between the variance and bias of the model. Therefore, it deals with the chances both of overfitting and underfitting. The model with the lowest AIC score is preferred over other models.\n",
        "[In]: model_predictions.aic\n",
        "[Out]: -1939.88\n",
        "We can run the GLM for multiple distributions, such as\n",
        "\n",
        "    1.\n",
        "\n",
        "    Binomial\n",
        "     \n",
        "    2.\n",
        "\n",
        "    Poisson\n",
        "     \n",
        "    3.\n",
        "\n",
        "    Gamma\n",
        "     \n",
        "    4.\n",
        "\n",
        "    Tweedie\n",
        "     \n",
        "\n",
        "[In]: glr = GeneralizedLinearRegression(family='Binomial')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.aic\n",
        "[Out]: 336.991\n",
        "[In]: glr = GeneralizedLinearRegression(family='Poisson')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]: predictions=glr_model.evaluate(test)\n",
        "[In]: predictions.aic\n",
        "[Out]: 266.53\n",
        "[In]: glr = GeneralizedLinearRegression(family='Gamma')\n",
        "[In]: glr_model = glr.fit(train)\n",
        "[In]:model_predictions=glr_model.evaluate(test)\n",
        "[In]: model_predictions.aic\n",
        "[Out]: -1903.81\n",
        "\n",
        "Here we can see that our default GLM model with Gaussian distribution has the lowest AIC value, compared to others.\n",
        "Decision Tree Regression\n",
        "\n",
        "The decision tree regression algorithm can be used for both regression and classification. It is quite powerful in terms of fitting the data well but comes with the high risk of sometimes overfitting the data. Decision trees contain multiple splits based on entropy or Gini indexes. The deeper the tree, the higher the chance of overfitting the data. In our example, we will build a decision tree for predicting the target value, with the default value of parameters (maxdepth = 5).\n",
        "Step 1: Build and Train Decision Tree Regressor Model\n",
        "[In]: from pyspark.ml.regression import DecisionTreeRegressor\n",
        "[In]: dec_tree = DecisionTreeRegressor()\n",
        "[In]: dec_tree_model = dec_tree.fit(train)\n",
        "[In]: dec_tree_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.9641, 1: 0.0193, 2: 0.0029, 3: 0.0053, 4: 0.0084})\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "[In]: model_predictions = dec_tree_model.transform(test)\n",
        "[In]: model_predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figf_HTML.jpg\n",
        "We import RegressionEvaluation from MLlib, to evaluate the performance of the decision tree on test data. As of now, there are two metrics available for evaluation: r2 and RMSE (root mean squared error). r2 mainly suggests how much of the variation in the dataset can be attributed to regression. Therefore, the higher the r2, the better the performance of the model. On the other hand, RMSE suggests the total errors the model is making, in terms of the difference between actual and predicted values.\n",
        "[In]: from pyspark.ml.evaluation import RegressionEvaluator\n",
        "[In]: dt_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: dt_r2 = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of DecisionTreeRegressor is {dt_r2}')\n",
        "[Out]: The r-square value of DecisionTreeRegressor is 0.8093834699203476\n",
        "[In]: dt_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: dt_rmse = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of DecisionTreeRegressor is {dt_rmse}')\n",
        "[Out]: The rmse value of DecisionTreeRegressor is 0.014111932287681688\n",
        "\n",
        "The r2 value of this particular model is close to 0.81, which is a little lower than that of a simple linear regression model.\n",
        "Random Forest Regressors\n",
        "\n",
        "Random forest regressors are a collection of multiple individual decision trees built using different samples of data. The whole idea of combining these individual trees is to take majority voting or averages (in case of regression) to generalize effectively. A random forest is, therefore, an ensembling technique that takes a bagging approach. It can be used for regression as well as for classification tasks. Because decision trees tend to overfit the data, random forests remove the element of high variance, by taking the means of the predicted values from individual trees. In our example, we will build a random forest model for regression, using default parameters (numTrees = 20)\n",
        "Step 1: Build and Train Random Forest Regressor Model\n",
        "[In]: from pyspark.ml.regression import RandomForestRegressor\n",
        "[In]: rf = RandomForestRegressor()\n",
        "[In]: rf_model = rf.fit(train)\n",
        "[In]:  rf_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.4395, 1: 0.045, 2: 0.0243, 3: 0.2725, 4: 0.2188})\n",
        "As you can see, the number of trees in the random forest is equal to 20. This number can be increased.\n",
        "[In]: rf_model.getNumTrees\n",
        "[Out]: 20\n",
        "[In]: model_predictions = rf_model.transform(test)\n",
        "[In]: model_predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figg_HTML.jpg\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "We can again use r2and RMSE as the evaluation parameter of the random forest model.\n",
        "[In]:rf_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: rf_r2 = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of RandomForestRegressor is {rf_r2}')\n",
        "[Out]: The r-square value of RandomForestRegressor is 0.8215863293044671\n",
        "[In]: rf_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: rf_rmse = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of RandomForestRegressor is {rf_rmse}')\n",
        "[Out]: The rmse value of RandomForestRegressor is 0.01365275410722947\n",
        "\n",
        "As you can see, it clearly outperforms the decision tree regressor and has a higher r2. The performance of this model can further be enhanced with hyperparameter tuning.\n",
        "Gradient-Boosted Tree Regressor\n",
        "\n",
        "A gradient-boosted tree (GBT) regressor is also an ensembling technique, which uses boosting under the hood. Boosting refers to making use of individual weak learners in order to boost the performance of the overall model. One major difference between bagging and boosting is that in bagging, the individual models that are built are parallel in nature, meaning they can be built independent of each other, but in boosting, the individual models are built in a sequential manner. In a gradient boosting approach, the second model focuses on the errors made by the first model and tries to reduce overall errors for those data points. Similarly, the next model tries to reduce the errors made by the previous model. In this way, the overall error of prediction is reduced. In the following example, we will build a GBT regressor with default parameters.\n",
        "Step 1: Build and Train a GBT Regressor Model\n",
        "[In]: from pyspark.ml.regression import GBTRegressor\n",
        "[In]: gbt = GBTRegressor()\n",
        "[In]: gbt_model=gbt.fit(train)\n",
        "[In]: gbt_model.featureImportances\n",
        "[Out]: SparseVector(5, {0: 0.2325, 1: 0.2011, 2: 0.1645, 3: 0.2268, 4: 0.1751})\n",
        "[In]: model_predictions = gbt_model.transform(test)\n",
        "[In]: model_predictions.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figh_HTML.jpg\n",
        "Step 2: Evaluate the Model Performance on Test Data\n",
        "[In]: gbt_evaluator = RegressionEvaluator(metricName='r2')\n",
        "[In]: gbt_r2 = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The r-square value of GradientBoostedRegressor is {gbt_r2}')\n",
        "[Out]: The r-square value of GradientBoostedRegressor is 0.8477273892307596\n",
        "[In]: gbt_evaluator = RegressionEvaluator(metricName='rmse')\n",
        "[In]: gbt_rmse = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The rmse value of GradientBoostedRegressor is {gbt_rmse}')\n",
        "[Out]: The rmse value of GradientBoostedRegressor is 0.013305445803592103\n",
        "\n",
        "As you can see, the GBT regressor outperforms the random forestmodel. With r2 being close to 0.85, it can be considered the final model, after proper tuning.\n",
        "Building Multiple Models for Binary Classification Tasks\n",
        "\n",
        "In this third and final section of the chapter, you will see how to build multiple machine learning models for binary classification tasks. The data that we are going to use for this is a subset of an open source Bank Marketing Data Set from the UCI ML repository, available at https://archive.ics.uci.edu/ml/datasets/Bank+Marketing.\n",
        "\n",
        "There are two reasons for selecting only a subset of this data. The first is to maintain the class balance for the classification task, so as not to make it an anomalous detection category task. Another reason for selecting only a subset of the features is to limit the amount of signals in the data, as some of the features in the dataset strongly affect the output and, therefore, are ignored in this exercise.\n",
        "\n",
        "The dataset contains 9,500 rows and 8 columns. The idea is to predict if the user will subscribe to another product or service (term deposit), based on the other attributes, such as age, job, loan, etc. This is a typical requirement in which machine learning is leveraged to find the top users who can be targeted by the business for cross-selling or upselling.\n",
        "\n",
        "I’ll begin with the logistic regression model.\n",
        "Logistic Regression\n",
        "\n",
        "Logistic regression is considered to be one of baseline models, owing to its simplicity and interpretability. Under the hood, it is quite similar to linear regression. It also assumes that output is a linear combination of the dependent variables, but to keep the output between 0 and 1, as it returns the probability as output, it makes use of a nonlinear function (sigmoid), which produces an S curve instead of a straight line (linear regression).\n",
        "\n",
        "We’ll start by building the baseline in Steps 1–3 and then complete the logistic regression model with default hyperparameters, in Steps 4–6.\n",
        "Step 1: Read the Dataset\n",
        "[In]: df=spark.read.csv('bank_data.csv',inferSchema=True,header=True)\n",
        "[In]: df.count()\n",
        "[Out]: 9501\n",
        "[In]: df.columns\n",
        "[Out]: ['age', 'job', 'marital', 'education', 'default', 'housing','loan', 'target_class']\n",
        "[In]: df.printSchema()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figi_HTML.jpg\n",
        "As you can see, the input columns are all the columns, except for the target class column. The target class is also well-balanced, in terms of the count of yes and no labels. We will have to convert yeses and noes into 1s and 0s, as well as rename the target_class column to “label,” which is the default acceptance column name in machine learning model parameters.\n",
        "[In]: df.groupBy('target_class').count().show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figj_HTML.jpg\n",
        "Step 2: Feature Engineering for Model\n",
        "[In]: from pyspark.sql import functions as F\n",
        "[In]: from pyspark.sql import ∗\n",
        "[In]: df=df.withColumn(\"label\", F.when(df.target_class =='no', F.lit(0)).otherwise(F.lit(1)))\n",
        "[In]: df.groupBy('label').count().show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figk_HTML.jpg\n",
        "Now that we have renamed the output column “label” and converted the target class to 1s and 0s, the next step is to create features for the model. Because we have categorical columns, such as job and edu, we will have to use StringIndexer and OneHotEncoder to convert them into a numerical format. We create a Python function, cat_to_num, to convert all the categorical features into numerical ones.\n",
        "[In]: from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "[In]: def cat_to_num(df):\n",
        "    for col in df.columns:\n",
        "        stringIndexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n",
        "        model = stringIndexer.fit(df)\n",
        "        indexed = model.transform(df)\n",
        "        encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\")\n",
        "        df = encoder.transform(indexed)\n",
        "    df_assembler = VectorAssembler(inputCols=['age','marital_vec','education_vec','default_vec','housing_vec','loan_vec'], outputCol=\"features\")\n",
        "    df = df_assembler.transform(df)\n",
        "    return df.select(['features','label'])\n",
        "We just select the new features column and target label column, as we don’t need the earlier original columns for model training.\n",
        "[In]: df_new=cat_to_num(df)\n",
        "[In]: df_new.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figl_HTML.jpg\n",
        "\n",
        "Now we have all the input features merged into a single dense vector ('features'), along with output column labels, which we can use to train the machine learning models. The new dataframe created using only two columns (features, label) is now called df_new and will be used for every model. We can now split this new dataframe into train and test datasets. We can split the data into a 75%/25% ratio, using the randomplit function.\n",
        "Step 3: Split the Data into Train and Test Datasets\n",
        "[In]: train, test = df_new.randomSplit([0.75, 0.25])\n",
        "[In]: print(f\"Size of train Dataset : {train.count()}\" )\n",
        "[Out]: 7121\n",
        "[In]: print(f\"Size of test Dataset : {test.count()}\" )\n",
        "[Out]: 2380\n",
        "Step 4: Build and Train the Logistic Regression Model\n",
        "[In]: from pyspark.ml.classification import LogisticRegression\n",
        "[In]: lr = LogisticRegression()\n",
        "[In]: lr_model = lr.fit(train)\n",
        "[In]:print( lr_model.coefficients)\n",
        "[Out]:\n",
        "[0.0272019114172,-0.647672064875,0.229030508111,-0.77074788287,-12.36869511,-12.8865599132,-13.2257790609,-12.6705131313,-13.0023164274,-13.0747662586,-12.6985757761,1.42220523957,0.301582233094,-0.0127231892838,0.218471149577,0.332362933568]\n",
        "\n",
        "Once the model is built, we can make use of the internal function summary, which offers important details regarding the model, such as ROC curve, precision, recall, AUC (area under the curve), etc.\n",
        "Step 5: Evaluate Performance on Training Data\n",
        "[In]: lr_summary=lr_model.summary\n",
        "[In]: lr_summary.accuracy\n",
        "[Out]: 0.673079623648364\n",
        "[In]: lr_summary.areaUnderROC\n",
        "[Out]: 0.7186044694483512\n",
        "[In]: lr_summary.weightedRecall\n",
        "[Out]: 0.673079623648364\n",
        "[In]: lr_summary.weightedPrecision\n",
        "[Out]: 0.6750967624018298\n",
        "Here, using the summary function , we can view the model’s performance on train data, such as its accuracy, AUC, weighted recall, and precision. We can also view additional details—such as how precision varies for various threshold values, the relation between precision and recall, and how recall varies with different threshold values—to pick the right threshold value for the model. These also can be plotted, to view the relationships.\n",
        "[In]: lr_summary.precisionByThreshold.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figm_HTML.jpg\n",
        "[In]: lr_summary.roc.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fign_HTML.jpg\n",
        "[In]: lr_summary.recallByThreshold.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figo_HTML.jpg\n",
        "[In]: lr_summary.pr.show()\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figp_HTML.jpg\n",
        "Step 6: Evaluate Performance on Test Data\n",
        "[In]: model_predictions = lr_model.transform(test)\n",
        "[In]: model_predictions.columns\n",
        "[Out]: ['features', 'label', 'rawPrediction', 'probability', 'prediction']\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figq_HTML.jpg\n",
        "As you can see, the prediction column shows the model prediction for each of the records in the test data. The probability column shows the values for both classes (0 & 1). The probability at 0th index is of 0; the other is for a prediction of 1. The evaluation of the logistic regression model on test data can be done using BinaryClassEvaluator . We can get the area under ROC and that under the PR curve, as shown following:\n",
        "[In]:from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "[In]:lr_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: lr_auroc = lr_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auroc value of Logistic Regression Model is {lr_auroc}')\n",
        "[Out]: The auroc value of Logistic Regression Model is 0.7092938229110143\n",
        "[In]: lr_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: lr_aupr = lr_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of Logistic Regression Model is {lr_aupr}')\n",
        "[Out]: The aupr value of Logistic Regression Model is 0.6630743130940658\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "Recall\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.6701030927835051\n",
        "Precision\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.6478405315614618\n",
        "Decision Tree Classifier\n",
        "\n",
        "As mentioned earlier, decision trees can be used for classification as well as regression. Here, we will build a decision tree with default hyperparameters and use it to predict whether the user will opt for the new term deposit plan.\n",
        "Step 1: Build and Train Decision Tree Classifier Model\n",
        "[In]: from pyspark.ml.classification import DecisionTreeClassifier\n",
        "[In]: dt = DecisionTreeClassifier()\n",
        "[In]: dt_model = dt.fit(train)\n",
        "[In]: model_predictions = dt_model.transform(test)\n",
        "[Out]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figr_HTML.jpg\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: dt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: dt_auroc = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of Decision Tree Classifier Model is {dt_auroc}')\n",
        "[Out]: The auc value of Decision Tree Classifier Model is 0.516199386190993\n",
        "[In]: dt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: dt_aupr = dt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of Decision Tree Model is {dt_aupr}')\n",
        "[Out]: The aupr value of Decision Tree Model is 0.46771834172588167\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.6907216494845361\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.6661143330571665\n",
        "Support Vector Machines Classifiers\n",
        "Support vector machines (SVMs) are used for classification tasks, as they find the hyperplane that maximizes the margin (perpendicular distance) between two classes. All the instances and target classes are represented as vectors in high-dimensional space, and the SVM finds the closest two points from the two classes that support the best separating line or hyperplane, as shown in Figure 6-5.\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Fig5_HTML.jpg\n",
        "Figure 6-5\n",
        "\n",
        "Support vector machine\n",
        "\n",
        "For nonlinearly separable data, there are different kernel tricks to separate the classes. In our example, we will build a linearly separable support vector classifier with default hyperparameters.\n",
        "Step 1: Build and Train SVM Model\n",
        "[In]: from pyspark.ml.classification import LinearSVC\n",
        "[In]: lsvc = LinearSVC()\n",
        "[In]: lsvc_model = lsvc.fit(train)\n",
        "[In]: model_predictions = lsvc_model.transform(test)\n",
        "[In]: model_predictions.columns\n",
        "[Out]: ['features', 'label', 'rawPrediction', 'prediction']\n",
        "[In]:model_predictions.select(['label','prediction']).show(10,False)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figs_HTML.jpg\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: svc_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: svc_auroc = svc_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of SupportVectorClassifier  is {svc_auroc}')\n",
        "[Out]: The auc value of SupportVectorClassifier  is 0.7043772749366973\n",
        "[In]: svc_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: svc_aupr =svc_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of SupportVectorClassifier Model is {svc_aupr}')\n",
        "[Out]: The aupr value of SupportVectorClassifier Model is 0.6567277377856992\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.7774914089347079\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.600132625994695\n",
        "Naive Bayes Classifier\n",
        "\n",
        "Naive Bayes (NB) classifiers work on the principle of conditional probability and assume absolute independence between predictors. An NB classifier doesn’t have many hyperparameters and can outperform some of the most sophisticated algorithms out there. In the following example, we will build an NB classifier and evaluate its performance on the test data.\n",
        "Step 1: Build and Train SVM Model\n",
        "[In]: from pyspark.ml.classification import NaiveBayes\n",
        "[In]: nb = NaiveBayes()\n",
        "[In]: nb_model = nb.fit(train)\n",
        "[In]: model_predictions = nb_model.transform(test)\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figt_HTML.jpg\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: nb_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: nb_auroc = nb_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of NB Classifier is {nb_auroc}')\n",
        "[Out]: The auc value of NB Classifier is 0.43543736717760884\n",
        "[In]: nb_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: nb_aupr =nb_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of NB Classifier Model is {nb_aupr}')\n",
        "[Out]: The aupr value of NB Classifier Model is 0.4321001351769349\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.586\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.625\n",
        "Gradient Boosted Tree Classifier\n",
        "\n",
        "So far, we have used single algorithms for classification. Now we move on to use ensemble methods, such as GBT and random forests, for classification. Bagging and boosting for classification works according to similar principles as regression.\n",
        "Step 1: Build and Train the GBT Model\n",
        "[In]: from pyspark.ml.classification import GBTClassifier\n",
        "[In]: gbt = GBTClassifier()\n",
        "[In]: gbt_model = gbt.fit(train)\n",
        "[In]: model_predictions = gbt_model.transform(test)\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "[Out]:\n",
        "../images/478390_1_En_6_Chapter/478390_1_En_6_Figu_HTML.jpg\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: gbt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: gbt_auroc = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of GradientBoostedTreesClassifier is {gbt_auroc}')\n",
        "[Out]: The auc value of GradientBoostedTreesClassifier is 0.7392410330756018\n",
        "[In]: gbt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: gbt_aupr = gbt_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of GradientBoostedTreesClassifier Model is {gbt_aupr}')\n",
        "[Out]: The aupr value of GradientBoostedTreesClassifier Model is 0.7345982892755392\n",
        "[In]: true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.668\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.674\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kMJwAE0M24C"
      },
      "source": [
        "Step 1: Build and Train the Random Forest Model\n",
        "[In]: from pyspark.ml.classification import RandomForestClassifier\n",
        "[In]: rf = RandomForestClassifier(numTrees=50,maxDepth=30)\n",
        "[In]: rf_model = rf.fit(train)\n",
        "[In]: model_predictions=rf_model.transform(test)\n",
        "[In]: model_predictions.select(['label','probability','prediction']).show(10,False)\n",
        "\n",
        "Step 2: Evaluate Performance on Test Data\n",
        "[In]: rf_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: rf_auroc = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The auc value of RandomForestClassifier Model is {rf_auroc}')\n",
        "[Out]:\n",
        "The auc value of RandomForestClassifier Model is 0.7326433634020617\n",
        "[In]: rf_evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
        "[In]: rf_aupr = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(f'The aupr value of RandomForestClassifier Model is {rf_aupr}')\n",
        "[Out]: The aupr value of RandomForestClassifier Model is 0.7277253895494864\n",
        "[In]; true_pos=model_predictions.filter(model_predictions['label']==1).filter(model_predictions['prediction']==1).count()\n",
        "[In]: actual_pos=model_predictions.filter(model_predictions['label']==1).count()\n",
        "[In]: pred_pos=model_predictions.filter(model_predictions['prediction']==1).count()\n",
        "[In]: float(true_pos)/(actual_pos)\n",
        "[Out]: 0.67\n",
        "[In]: float(true_pos)/(pred_pos)\n",
        "[Out]: 0.67\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnNAPxARM26B"
      },
      "source": [
        "#Hyperparameter Tuning and Cross-Validation\n",
        "\n",
        "\n",
        "\n",
        "[In]: from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "[In]: rf = RandomForestClassifier()\n",
        "[In]: paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [5,10,20,25,30])\n",
        "             .addGrid(rf.maxBins, [20, 60])\n",
        "             .addGrid(rf.numTrees, [5, 20,50,100])\n",
        "             .build())\n",
        "[In]: cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=rf_evaluator, numFolds=5)\n",
        "[In]: cv_model = cv.fit(train)\n",
        "[In]: best_rf_model = cv_model.bestModel\n",
        "best_rf_model contains the best hyper-parameters to be used for training the model on this dataset.\n",
        "[In]: model_predictions = best_rf_model.transform(test)\n",
        "[In]: rf_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "[In]: rf_auroc = rf_evaluator.evaluate(model_predictions)\n",
        "[In]: print(rf_auroc)\n",
        "[Out]:  0.7425990374615659"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTy7x7PNM28w"
      },
      "source": [
        "#Unsupervised ML\n",
        "\n",
        "df = spark.read.csv('/content/learn-pyspark/chap_7/music_data.csv',header=True,inferSchema=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2E_WCAyMOlS",
        "outputId": "42efb7a4-7343-4b4f-946b-87d3273478d0"
      },
      "source": [
        "df.show(3,truncate=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------+----------------+\n",
            "|user_id                                                         |music category  |\n",
            "+----------------------------------------------------------------+----------------+\n",
            "|05c388ffc3e28f2419fa8f4f0a4cdebfadb48a425bfc2a3a28115bc521911a17|Motivational    |\n",
            "|05c388ffc3e28f2419fa8f4f0a4cdebfadb48a425bfc2a3a28115bc521911a17|Blues           |\n",
            "|05c388ffc3e28f2419fa8f4f0a4cdebfadb48a425bfc2a3a28115bc521911a17|Electronic Music|\n",
            "+----------------------------------------------------------------+----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd8lTkymQyo7",
        "outputId": "1580069d-55fc-42e7-9bc2-4a160e9d8032"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- music category: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heSzivAGQz2A"
      },
      "source": [
        "import pyspark\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.clustering import KMeans"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjUCyA39RFQN",
        "outputId": "19302b18-0ba3-42d4-d23b-dd0e303e1b18"
      },
      "source": [
        "df.select('music category').distinct().count()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJrvTH4GRShR",
        "outputId": "e2211a13-54e3-4676-af97-f07c061ba561"
      },
      "source": [
        "feature = df.stat.crosstab('user_id','music category')\n",
        "feature.show(3)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+\n",
            "|user_id_music category|Alternative Music|Asian Pop|Blues|Classical Music|Country Music|Dance Music|Easy Listening|Electronic Music|Hindi Music|Hip Hop|Indie Pop|Jazz|Latin Music|Motivational|New Age|Opera|Popular|Rap|Reggae|Rock|Soulful|\n",
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+\n",
            "|  ca3c4285512798abe...|               65|        0|    7|              8|            7|         12|             7|               2|         12|      7|        0|  15|          0|           2|      0|    3|      0|  3|     0|   1|      0|\n",
            "|  b457919ae168ac12e...|               80|        0|   65|            135|           16|         55|             7|              34|          0|     12|        6|   7|          3|           2|      1|    2|      0|  5|     0|   0|      0|\n",
            "|  a87bb168ba6e5a8da...|                0|        0|   11|              0|            9|          3|             0|               3|          3|      0|        5|   0|          0|           1|      0|    0|      4|  0|     0|   0|      5|\n",
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXI2P3XcRzu9"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "feat_cols = [ col for col in feature.columns if col != 'user_id_music category']"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QDHzmrRSIYf"
      },
      "source": [
        "vec_assembler = VectorAssembler(inputCols=feat_cols,outputCol='features').transform(feature)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_rXsZ5bTe_R",
        "outputId": "3221fbad-62a6-4554-b784-fe776c8d34b6"
      },
      "source": [
        "vec_assembler.show(3)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+--------------------+\n",
            "|user_id_music category|Alternative Music|Asian Pop|Blues|Classical Music|Country Music|Dance Music|Easy Listening|Electronic Music|Hindi Music|Hip Hop|Indie Pop|Jazz|Latin Music|Motivational|New Age|Opera|Popular|Rap|Reggae|Rock|Soulful|            features|\n",
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+--------------------+\n",
            "|  ca3c4285512798abe...|               65|        0|    7|              8|            7|         12|             7|               2|         12|      7|        0|  15|          0|           2|      0|    3|      0|  3|     0|   1|      0|[65.0,0.0,7.0,8.0...|\n",
            "|  b457919ae168ac12e...|               80|        0|   65|            135|           16|         55|             7|              34|          0|     12|        6|   7|          3|           2|      1|    2|      0|  5|     0|   0|      0|[80.0,0.0,65.0,13...|\n",
            "|  a87bb168ba6e5a8da...|                0|        0|   11|              0|            9|          3|             0|               3|          3|      0|        5|   0|          0|           1|      0|    0|      4|  0|     0|   0|      5|(21,[2,4,5,7,8,10...|\n",
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOMq5fu9SX8Y"
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler(inputCol='features',outputCol=\"scaledFeatures\", \n",
        "                        withStd=True, withMean=False).fit(vec_assembler).transform(vec_assembler)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiYhwTjdTV-1",
        "outputId": "8f1437bb-843a-4bd8-c472-e68aa504942d"
      },
      "source": [
        "scaler.show(3)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+--------------------+--------------------+\n",
            "|user_id_music category|Alternative Music|Asian Pop|Blues|Classical Music|Country Music|Dance Music|Easy Listening|Electronic Music|Hindi Music|Hip Hop|Indie Pop|Jazz|Latin Music|Motivational|New Age|Opera|Popular|Rap|Reggae|Rock|Soulful|            features|      scaledFeatures|\n",
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+--------------------+--------------------+\n",
            "|  ca3c4285512798abe...|               65|        0|    7|              8|            7|         12|             7|               2|         12|      7|        0|  15|          0|           2|      0|    3|      0|  3|     0|   1|      0|[65.0,0.0,7.0,8.0...|[0.23836018560172...|\n",
            "|  b457919ae168ac12e...|               80|        0|   65|            135|           16|         55|             7|              34|          0|     12|        6|   7|          3|           2|      1|    2|      0|  5|     0|   0|      0|[80.0,0.0,65.0,13...|[0.29336638227904...|\n",
            "|  a87bb168ba6e5a8da...|                0|        0|   11|              0|            9|          3|             0|               3|          3|      0|        5|   0|          0|           1|      0|    0|      4|  0|     0|   0|      5|(21,[2,4,5,7,8,10...|(21,[2,4,5,7,8,10...|\n",
            "+----------------------+-----------------+---------+-----+---------------+-------------+-----------+--------------+----------------+-----------+-------+---------+----+-----------+------------+-------+-----+-------+---+------+----+-------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cIosuMCUBKP"
      },
      "source": [
        "from pyspark.sql.functions import * \n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import rand, randn\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.functions import sha2\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C-xL9oATmyB",
        "outputId": "22736ece-b0b1-4832-bb23-7ea7cf5bb07f"
      },
      "source": [
        "errors=[]\n",
        "for k in range(2,10):\n",
        "  kmeans = KMeans(featuresCol='scaledFeatures',k=8)\n",
        "  model = kmeans.fit(scaler)\n",
        "  evaluator = ClusteringEvaluator()\n",
        "  validate=evaluator.evaluate(model.transform(scaler))\n",
        "  print(k,'-->',evaluator.evaluate(model.transform(scaler)))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 --> 0.7901472797049975\n",
            "3 --> 0.7901472797049975\n",
            "4 --> 0.7901472797049975\n",
            "5 --> 0.7901472797049975\n",
            "6 --> 0.7901472797049975\n",
            "7 --> 0.7901472797049975\n",
            "8 --> 0.7901472797049975\n",
            "9 --> 0.7901472797049975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdgASrISUDIT",
        "outputId": "5488a2ee-85de-4be9-fed8-9af8a12196da"
      },
      "source": [
        "kmeans6 = KMeans(featuresCol='scaledFeatures',k=6)\n",
        "model_k6 = kmeans6.fit(scaler)\n",
        "model_k6.transform(scaler).groupBy('prediction').count().show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         1|    1|\n",
            "|         3|    1|\n",
            "|         5|   38|\n",
            "|         4|    4|\n",
            "|         2|    4|\n",
            "|         0|  727|\n",
            "+----------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTOm_60RU7nH"
      },
      "source": [
        "cluser_prediction=model_k6.transform(scaler)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePYQJtnIXaJs"
      },
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "pca = PCA(k=3, inputCol=\"scaledFeatures\", outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(cluser_prediction)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kW5A93gX3rK"
      },
      "source": [
        "result = pca_model.transform(cluser_prediction).select('user_id_music category',\"pca_features\",'prediction')"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYiUatWbX3zi",
        "outputId": "a7e0e98c-e38a-4bb5-d8b9-17a659e2082f"
      },
      "source": [
        "result.show(truncate=False)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------+----------------------------------------------------------------+----------+\n",
            "|user_id_music category                                          |pca_features                                                    |prediction|\n",
            "+----------------------------------------------------------------+----------------------------------------------------------------+----------+\n",
            "|ca3c4285512798abe7d81fa2d0588549211be9516d9b2050e5564f8e912312c7|[-0.2250735026516283,0.15082893636632264,-0.13922132997808168]  |0         |\n",
            "|b457919ae168ac12ec7e0303ae0d5fe292b63c9d04eb54e039942e7f46552ec4|[-0.5273003162834194,0.10206886950980101,-0.014971360310926076] |0         |\n",
            "|a87bb168ba6e5a8da5eccce18c95565d843b40c1833b0f6c3089eb083f0fe3d3|[-0.23968261020295822,0.2801153618243113,-0.17382194436381926]  |0         |\n",
            "|cf60c9b1a0444261c3a4687677f5336686b96756df5e33c11f6016f16fb38372|[-3.512051591993015,0.327294331895736,1.1017302617933258]       |0         |\n",
            "|8ab21c1b361d5c4aa1ed01893305febf1c6659bd342217eece1225d789ba57da|[-0.6796385576703685,0.3960805354648404,0.1147408585705326]     |0         |\n",
            "|34e22754ffec4a0d434edfa0f3b10c0e1ab14c632cee37514076d211cf1e7017|[-2.096405032936416,0.912132003547419,0.9912613846332528]       |0         |\n",
            "|ad3ea6999df3120ccb557d34cf75e17dfdd3febe445e9e1bc36514642fcd3bdb|[-0.2944449488892165,0.38397804726513174,0.07758752095208707]   |0         |\n",
            "|3d2ea50eeeb98d4a79259ee057955696ec9dc46e1b72e8d8eff099feb3362e1f|[-0.9356027216181405,0.5958240577216134,-0.17524154004372683]   |0         |\n",
            "|334533d21eeed37dd2b686022857f9f42eef9fa6192a8653040af844be8d15ef|[-0.13886158277802202,0.15784222493276162,-0.11260624144367234] |0         |\n",
            "|bad9c123790cfd0b1106190de5623afbb70de0e71f5d48dc05ee6d743a2f50a8|[-0.5824585609741291,0.6698195803354228,-0.8684054960184704]    |0         |\n",
            "|dd8b41323bbef6d4093879c63966b025d9dd3dbdab1b372ec3c09ee9e40e277c|[-0.19613038529370597,0.12812984441339167,-0.09549024406090134] |0         |\n",
            "|ac40c16b3c6248cebdfb8cb9e2dee4575862ef1223ece1b393b45b6b2561e8fc|[-0.28548897643969695,0.15074842985347456,-0.255603590895991]   |0         |\n",
            "|57c20a25da427c79b41690be1b895110bbae9fb583a32a2d27c310e9caa1a395|[-0.11298754622944401,-0.02683169465747659,0.012421222276786055]|0         |\n",
            "|b5773ac4aa0d079266ecc3f02d0a16a0f7fd0789b7c0553782dd0588d57298b4|[-0.49180214506782777,0.3841542970784486,-0.10281962873038585]  |0         |\n",
            "|793c0773daa478bf340a4c6f63fad3beedef69aaa0a8f503995450c8a75cd2bd|[-0.059303805689035136,0.03756596949668744,0.0826018132544698]  |0         |\n",
            "|4c52a409ecbf4c78bf6aa73611615d7c68090162250aeed3118d57177e291e63|[-1.4580702292222274,1.8999456814098974,-3.7715011579509863]    |0         |\n",
            "|fadebf953475576be81797433f0d28278858f93eee0cefcc95b5b03dd787c327|[-0.7083739895738097,0.21507912946609586,0.3575382191220823]    |0         |\n",
            "|86f98ea95c8b3333b947bf9a1a305150633df37e6aac807cddcbbe3463bfd53d|[-2.465870896553219,0.6606526791365824,0.32230865842212647]     |0         |\n",
            "|1a9da82f18b16577b872a71cefdeae63a7725eac5547820182ba82d8845b02ee|[-4.316559639888829,2.1119157004751306,1.549348692111248]       |5         |\n",
            "|9e528e07d7deddba5dcafd78c60059c7a91a7233350b2dcad109ae3b413d6aaf|[-0.8673164731150799,0.02574850968955722,0.2245798415193418]    |0         |\n",
            "+----------------------------------------------------------------+----------------------------------------------------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GT0jfiEX5Zz"
      },
      "source": [
        "clusters = result.toPandas().set_index('user_id_music category')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rJ3cHrUYPQQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj1eF59-Yb9C"
      },
      "source": [
        "#Deep Learning Using PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "9H5nqngdYc5h",
        "outputId": "821707e1-5129-41ca-ad4c-5d0e2acc1119"
      },
      "source": [
        "spark"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://db82aa79afe5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>atom</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd6dd4ab150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OANGZgY5bf6z"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import *\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU0Ls5sWcSAc"
      },
      "source": [
        "df = spark.read.csv('/content/learn-pyspark/chap_8/dl_data.csv',inferSchema=True,header=True)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3aWyaP_cbkr",
        "outputId": "e9788b24-e060-4565-fb83-f3570657f5e8"
      },
      "source": [
        "df.show(3)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+---------------------+-----------------+-------------------------------------+-------------------------------+-----------------------+------------------------------+-------------------------------+-------------------------+-------------------------+-------------------+------------------+-------------------+-----------------+------------------+------------------+-------------------------+\n",
            "|Visit_Number_Bucket|Page_Views_Normalized|Orders_Normalized|Internal_Search_Successful_Normalized|Internal_Search_Null_Normalized|Email_Signup_Normalized|Total_Seconds_Spent_Normalized|Store_Locator_Search_Normalized|Mapped_Last_Touch_Channel|Mapped_Mobile_Device_Type|Mapped_Browser_Type|Mapped_Entry_Pages|Mapped_Site_Section|Mapped_Promo_Code|Maped_Product_Name|Mapped_Search_Term|Mapped_Product_Collection|\n",
            "+-------------------+---------------------+-----------------+-------------------------------------+-------------------------------+-----------------------+------------------------------+-------------------------------+-------------------------+-------------------------+-------------------+------------------+-------------------+-----------------+------------------+------------------+-------------------------+\n",
            "|      less than  11|          0.026315789|                1|                                  0.0|                            0.0|                    0.0|                     4.5819E-4|                            0.0|                 Channel1|                  Device1|           Browser1|       Entry_Page1|      Site_Section1|      Promo_Code1|     Product_Name1|      Search_Term1|      Product_Collection1|\n",
            "|      less than  11|                  0.0|                1|                                  0.0|                            0.0|                    0.0|                           0.0|                            0.0|                 Channel1|                  Device1|           Browser2|       Entry_Page2|      Site_Section1|      Promo_Code1|     Product_Name1|      Search_Term1|      Product_Collection2|\n",
            "|      less than  11|                  0.0|                1|                                  0.0|                            0.0|                    0.0|                   0.002863688|                            0.0|                 Channel1|                  Device1|           Browser1|       Entry_Page2|      Site_Section1|      Promo_Code1|     Product_Name2|      Search_Term1|      Product_Collection3|\n",
            "+-------------------+---------------------+-----------------+-------------------------------------+-------------------------------+-----------------------+------------------------------+-------------------------------+-------------------------+-------------------------+-------------------+------------------+-------------------+-----------------+------------------+------------------+-------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2WaUuq0cdUY",
        "outputId": "ce4fdc61-5ce7-48f1-dc90-b408b16ff8db"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Visit_Number_Bucket: string (nullable = true)\n",
            " |-- Page_Views_Normalized: double (nullable = true)\n",
            " |-- Orders_Normalized: integer (nullable = true)\n",
            " |-- Internal_Search_Successful_Normalized: double (nullable = true)\n",
            " |-- Internal_Search_Null_Normalized: double (nullable = true)\n",
            " |-- Email_Signup_Normalized: double (nullable = true)\n",
            " |-- Total_Seconds_Spent_Normalized: double (nullable = true)\n",
            " |-- Store_Locator_Search_Normalized: double (nullable = true)\n",
            " |-- Mapped_Last_Touch_Channel: string (nullable = true)\n",
            " |-- Mapped_Mobile_Device_Type: string (nullable = true)\n",
            " |-- Mapped_Browser_Type: string (nullable = true)\n",
            " |-- Mapped_Entry_Pages: string (nullable = true)\n",
            " |-- Mapped_Site_Section: string (nullable = true)\n",
            " |-- Mapped_Promo_Code: string (nullable = true)\n",
            " |-- Maped_Product_Name: string (nullable = true)\n",
            " |-- Mapped_Search_Term: string (nullable = true)\n",
            " |-- Mapped_Product_Collection: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K0siaJTcfDv"
      },
      "source": [
        "df = df.withColumnRenamed('Orders_Normalized','label')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zpzT3HCcuZJ"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFV5UukKc9NJ"
      },
      "source": [
        "train, validation, test  = df.randomSplit([0.7, 0.2, 0.1], 1234)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQoMkqDec_ET"
      },
      "source": [
        "categorical_columns = [item[0] for item in df.dtypes if item[1].startswith('string')]\n",
        "numeric_columns = [item[0] for item in df.dtypes if item[1].startswith('double')]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX2isEvidJ8v"
      },
      "source": [
        "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(column)) for column in categorical_columns]\n",
        "featuresCreator = VectorAssembler(inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns, outputCol=\"features\")"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4DNcd4Ndacw",
        "outputId": "1d8d293e-fc9b-44f9-82bb-e569b5fbc6bf"
      },
      "source": [
        "layers = [len(featuresCreator.getInputCols()), 4, 2, 2]\n",
        "print(layers)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16, 4, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4zUe6lLeD3t"
      },
      "source": [
        "classifier = MultilayerPerceptronClassifier(labelCol='label', featuresCol='features',maxIter=100, layers=layers, blockSize=128, seed=1234)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lEtBA_VeXu-"
      },
      "source": [
        "pipeline = Pipeline(stages=indexers + [featuresCreator, classifier])"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F38DeTYZepad"
      },
      "source": [
        "model = pipeline.fit(train)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ubp1P5erDq"
      },
      "source": [
        "train_output_df = model.transform(train)\n",
        "validation_output_df = model.transform(validation)\n",
        "test_output_df = model.transform(test)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8hoDUt0e9z0"
      },
      "source": [
        "train_predictionAndLabels = train_output_df.select(\"prediction\", \"label\")\n",
        "validation_predictionAndLabels = validation_output_df.select(\"prediction\", \"label\")\n",
        "test_predictionAndLabels = test_output_df.select(\"prediction\", \"label\")"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLj-t2yGfFdT"
      },
      "source": [
        "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Ltqo8ZfIfj",
        "outputId": "6224f63c-dfd3-4828-e29d-bf9e71862c03"
      },
      "source": [
        "for metric in metrics:\n",
        "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
        "    print('Train ' + metric + ' = ' + str(evaluator.evaluate(train_predictionAndLabels)))\n",
        "    print('Validation ' + metric + ' = ' + str(evaluator.evaluate(validation_predictionAndLabels)))\n",
        "    print('Test ' + metric + ' = ' + str(evaluator.evaluate(test_predictionAndLabels)))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train weightedPrecision = 0.9718465803511076\n",
            "Validation weightedPrecision = 0.9724700548433042\n",
            "Test weightedPrecision = 0.9699435168283953\n",
            "Train weightedRecall = 0.9713846931436014\n",
            "Validation weightedRecall = 0.9719613447665714\n",
            "Test weightedRecall = 0.969260065288357\n",
            "Train accuracy = 0.9713846931436014\n",
            "Validation accuracy = 0.9719613447665714\n",
            "Test accuracy = 0.969260065288357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d60h3L_fK7R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}